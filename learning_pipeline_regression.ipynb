{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_pipeline_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julialromero/Collaborative-Problem-Solving/blob/main/learning_pipeline_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1g3ar0jiAuR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "BRAjwxfli5cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import os\n",
        "from skimage import io\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from keras import layers, Input, Model, optimizers\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "PROJECT_DIR = \"drive/MyDrive/CSCI 5922 - Final Project/\"  #<--- Make a shortcut for the \"CSCI 5922 - Final Project\" shared folder on your Google Drive"
      ],
      "metadata": {
        "id": "UN0E43_si6aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get User Input"
      ],
      "metadata": {
        "id": "s0pWhCIDjhIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose what size images to use\n",
        "PROJECT_DIR = \"drive/MyDrive/CSCI 5922 - Final Project/\"  #<--- Make a shortcut for the \"CSCI 5922 - Final Project\" shared folder on your Google Drive\n",
        "## \"small\" = 98x98, \"med\" = 332x332, \"large\" = 719x719\n",
        "image_size = \"med\"\n",
        "shuffled = False\n",
        "chance = False\n",
        "normalize = False\n",
        "num_iters = 10\n",
        "num_folds = 10\n",
        "threshold = 0.5\n",
        "num_epochs = 70\n",
        "\n",
        "if image_size == \"large\":\n",
        "  imgsize = (int(719/2), int(719/2)) \n",
        "  if shuffled:\n",
        "    IMG_DIR = \"719x719 - Recurrence_Matrices/RAW_SHUFFLED/\"\n",
        "  else:\n",
        "    IMG_DIR = \"719x719 - Recurrence_Matrices/RAW/\"\n",
        "\n",
        "elif image_size == \"med\":\n",
        "  imgsize = (332, 332)\n",
        "  if shuffled:\n",
        "    IMG_DIR = \"332x332 - Recurrence_Matrices/RAW_SHUFFLED/\"\n",
        "  else:\n",
        "    IMG_DIR = \"332x332 - Recurrence_Matrices/RAW/\"\n",
        "\n",
        "elif image_size == \"small\":\n",
        "  imgsize = (98, 98)\n",
        "  if shuffled:\n",
        "    IMG_DIR = \"98x98 - Recurrence_Matrices/RAW_SHUFFLED/\"\n",
        "  else:\n",
        "    IMG_DIR = \"98x98 - Recurrence_Matrices/RAW/\"\n",
        "\n",
        "print(\"Processing images from: \", IMG_DIR)"
      ],
      "metadata": {
        "id": "r-z42Z2RjiSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "BmoT19DNj5DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read, preprocess, and scale images to uniform dimension\n",
        "\n",
        "def extract_image(image_path):\n",
        "    imag = io.imread(image_path, as_gray=True)\n",
        "    x = image.img_to_array(imag)\n",
        "\n",
        "    x = tf.image.resize(x, imgsize) \n",
        "\n",
        "    return x\n",
        "\n",
        "'''\n",
        "    Converts grayscale image to 3 channels in order to fit Inception v3 model\n",
        "'''\n",
        "def extract_image_inceptionv3(image_path):\n",
        "    imag = io.imread(image_path, as_gray=True)\n",
        "    x = image.img_to_array(imag)\n",
        "\n",
        "    x = tf.image.resize(x, imgsize) \n",
        "    x = np.squeeze(np.stack((x,)*3))\n",
        "    x = x.swapaxes(0,1)\n",
        "    x = x.swapaxes(1,2)\n",
        "\n",
        "    return x\n",
        "\n",
        "def extract_score(filename, outcome_df, score_col=\"task_score\"):\n",
        "    #  match this file's id/block with task score and return\n",
        "    groupid, splitname = filename.split(\"-\")\n",
        "    block = splitname.split(\"_\")[0]\n",
        "    row = outcome_df.loc[(outcome_df.GROUPID == int(groupid)) & (outcome_df.block == block)]\n",
        "    if(row.shape[0] != 1):\n",
        "        print(f\"Error -- Number of task scores recorded is not 1 for block {block} and groupid {groupid}.\")\n",
        "        return np.nan\n",
        "    score = row[score_col].values[0]\n",
        "    return score\n",
        "\n",
        "\n",
        "def extract_binary_score(filename, outcome_df, score_col=\"task_score\", median_score=2):\n",
        "    #  match this file's id/block with task score and return\n",
        "    groupid, splitname = filename.split(\"-\")\n",
        "    block = splitname.split(\"_\")[0]\n",
        "    row = outcome_df.loc[(outcome_df.GROUPID == int(groupid)) & (outcome_df.block == block)]\n",
        "    if(row.shape[0] != 1):\n",
        "        print(f\"Error -- Number of task scores recorded is not 1 for block {block} and groupid {groupid}.\")\n",
        "        return np.nan\n",
        "    raw_score = row[score_col].values[0]\n",
        "    if raw_score <= median_score:\n",
        "        score = 0\n",
        "    else:\n",
        "        score = 1\n",
        "    return score\n",
        "\n",
        "def min_max_scaling(series):\n",
        "    # https://datagy.io/pandas-normalize-column/\n",
        "    return (series - series.min()) / (series.max() - series.min())\n",
        "\n",
        "def get_fold_data(GROUPID_list, incv3=False, verbose=False, score_col):\n",
        "  # read data to np arrays\n",
        "  recurrence_plot_list = []\n",
        "  if incv3:\n",
        "    recurrence_plot_list_incv3 = []\n",
        "  \n",
        "  labels = []\n",
        "  binary_labels = []\n",
        "\n",
        "  for filename in os.listdir(PROJECT_DIR + IMG_DIR):\n",
        "      # Check if file belongs to one of the teams in the desired group\n",
        "      for GROUPID in GROUPID_list:\n",
        "        if str(GROUPID) == filename.split(\"-\")[0]:\n",
        "          # preprocess image\n",
        "          if incv3:\n",
        "            t_incv3 = extract_image_inceptionv3(PROJECT_DIR + IMG_DIR + filename)\n",
        "          else:\n",
        "            t = extract_image(PROJECT_DIR + IMG_DIR + filename)\n",
        "\n",
        "          # get task score\n",
        "          lab = extract_score(filename, outcome_df, score_col)\n",
        "          bin_lab = extract_binary_score(filename, outcome_df, score_col=\"task_score\", median_score=2)\n",
        "\n",
        "          # append data \n",
        "          if incv3:\n",
        "            recurrence_plot_list_incv3.append(t_incv3)\n",
        "          else:\n",
        "            recurrence_plot_list.append(t)\n",
        "          \n",
        "          labels.append(lab)\n",
        "          binary_labels.append(bin_lab)\n",
        "\n",
        "  # convert to arrays \n",
        "  if incv3:\n",
        "    recurrence_plot_list_incv3 = np.array(recurrence_plot_list_incv3)\n",
        "    plot_list = recurrence_plot_list_incv3\n",
        "  else:\n",
        "    recurrence_plot_list = np.array(recurrence_plot_list)\n",
        "    plot_list = recurrence_plot_list\n",
        "  \n",
        "  labels = np.array(labels)\n",
        "  binary_labels = np.array(binary_labels)\n",
        "\n",
        "  if verbose:\n",
        "    print(\"\\nFOLD DATA INFO: \")\n",
        "    if incv3:\n",
        "      print(f\"\\trecurrence_plot_list_incv3: {recurrence_plot_list_incv3.shape}\")\n",
        "    else:\n",
        "      print(f\"\\trecurrence_plot_list: {recurrence_plot_list.shape}\")\n",
        "    \n",
        "    binary_counts = dict(Counter(binary_labels))\n",
        "    print(\"\\tBinary label counts: \", binary_counts)\n",
        "\n",
        "  return [plot_list, labels, binary_labels]\n",
        "  \n"
      ],
      "metadata": {
        "id": "njVlH7TOj6Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Team Outcomes"
      ],
      "metadata": {
        "id": "MJSniIOFjuNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outcome_df = pd.read_csv(PROJECT_DIR + \"team_block_outcomes.csv\")\n",
        "\n",
        "# get median task_score\n",
        "task_scores = outcome_df[\"task_score\"]\n",
        "median_task_score = np.median(task_scores)\n",
        "mean_task_score = np.mean(task_scores)\n",
        "print(\"median: \", median_task_score)\n",
        "print(\"mean: \", mean_task_score)\n",
        "\n",
        "# normalize task_score\n",
        "outcome_df[\"norm_task_score\"] = min_max_scaling(outcome_df[\"task_score\"])\n",
        "\n",
        "# score_counts = dict(Counter(task_scores))\n",
        "# for key in score_counts:\n",
        "#     print(\"%d: %d\" % (key, score_counts[key]))\n",
        "\n",
        "outcome_df"
      ],
      "metadata": {
        "id": "pPdt0o8MjvZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression w/ Team-level Cross Validation"
      ],
      "metadata": {
        "id": "G1tXo-RPmCIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep folds for team-level cross validation"
      ],
      "metadata": {
        "id": "-H9KHjoamQ6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fold names\n",
        "train_folds = []\n",
        "test_folds = []\n",
        "set_type = \"test\"\n",
        "for j in range(1,num_folds+1): \n",
        "    col_name = \"Fold\" + str(j) + \"_\" + set_type\n",
        "    test_folds.append(col_name)\n",
        "    set_type = \"train\"  \n",
        "    col_name = \"Fold\" + str(j) + \"_\" + set_type\n",
        "    train_folds.append(col_name)\n",
        "    set_type = \"test\"\n",
        "\n",
        "folds_dict_list = []\n",
        "\n",
        "# Split teams into 5 groups\n",
        "teams = pd.unique(outcome_df.GROUPID)\n",
        "\n",
        "# For every iteration\n",
        "for i in range(1,num_iters+1):\n",
        "    print(\"Iteration: \", i)\n",
        "    teams = shuffle(teams, random_state=i)\n",
        "    groups = np.array_split(teams, num_folds)\n",
        "    \n",
        "    # Define groups for each fold\n",
        "    fold_groups = {}\n",
        "    for j, (train_fold, test_fold) in enumerate(zip(train_folds, test_folds)):\n",
        "        # make the current group the test group\n",
        "        fold_groups[test_fold] = groups[j]\n",
        "        # make all other groups the train group\n",
        "        train_group = groups[:j] + groups[j+1:]\n",
        "        train_group = [team for group in train_group for team in group]\n",
        "        fold_groups[train_fold] = train_group\n",
        "        \n",
        "    ## Confirm that for each fold, there is no team overlap bewteen train and test set\n",
        "    for j in range(1,num_folds+1):\n",
        "        assert set(fold_groups['Fold'+str(j)+'_test']).isdisjoint(set(fold_groups['Fold'+str(j)+'_train'])), \"There is overlap in train and test set \" + str(j)\n",
        "    \n",
        "    print(\"* No team overlap *\")\n",
        "\n",
        "      \n",
        "    # Add fold groups to dictionary\n",
        "    folds_dict_list.append(fold_groups)\n",
        "    \n",
        "\n",
        "# Informational\n",
        "print(\"\\nNumber of iterations: \", len(folds_dict_list))\n",
        "\n",
        "print(\"\\nIterating through folds_dict_list to check for overlap...\")\n",
        "for i,dicti in enumerate(folds_dict_list):\n",
        "    for j in range(1,num_folds+1):\n",
        "        assert set(dicti['Fold'+str(j)+'_test']).isdisjoint(set(dicti['Fold'+str(j)+'_train'])), \"There is overlap in train and test set \" + str(j)\n",
        "    \n",
        "print(\"* No team overlap *\")  \n",
        "\n"
      ],
      "metadata": {
        "id": "djmgkOokmG-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Garcia model"
      ],
      "metadata": {
        "id": "DsWcvYrnme7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resource: https://www.kaggle.com/code/guidosalimbeni/regression-with-convolutional-neural-network-keras/notebook\n",
        "\n",
        "def get_reg_garcia_model():\n",
        "  # Garcia-Ceja paper feeds CNN images with dims 100 x 100 x 4 (width x height x channels)\n",
        "  # Labels are one hot encoded\n",
        "  inputs = Input(shape=(imgsize[0], imgsize[1], 1))\n",
        "  x = layers.Conv2D(filters=16, kernel_size=3, strides=(1, 1), activation=\"relu\")(inputs)\n",
        "  x = layers.Conv2D(filters=16, kernel_size=3, strides=(1, 1), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = layers.Dropout(0.25)(x)\n",
        "  x = layers.Conv2D(filters=32, kernel_size=3, strides=(1, 1), activation=\"relu\")(x)\n",
        "  x = layers.Conv2D(filters=32, kernel_size=3, strides=(1, 1), activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = layers.Dropout(0.25)(x)\n",
        "  x = layers.Dense(512, activation=\"relu\")(x) \n",
        "  x = layers.Dropout(0.50)(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  out = layers.Dense(1, activation=\"linear\")(x)   # TODO: Is this the proper output for regression?\n",
        "\n",
        "  lr = 0.00001 # paper used 0.001\n",
        "  eps = 1e-08  # paper used 1e-08, but keras default is 1e-07\n",
        "  adam = keras.optimizers.Adam(learning_rate=lr, epsilon=eps)\n",
        "\n",
        "  mse = tf.keras.metrics.MeanSquaredError()\n",
        "  rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "  mae = tf.keras.metrics.MeanAbsoluteError()\n",
        "  cos_sim = tf.keras.metrics.CosineSimilarity(axis=1)\n",
        "  \n",
        "  model_reg = Model(inputs=inputs, outputs=out)\n",
        "  model_reg.compile(\n",
        "    optimizer=adam,\n",
        "    loss=\"mse\", \n",
        "    metrics=['accuracy', mse, rmse, mae, cos_sim]\n",
        "  )\n",
        "\n",
        "  return model_reg\n",
        "\n",
        "model_reg = get_reg_garcia_model()\n",
        "model_reg.summary()\n"
      ],
      "metadata": {
        "id": "J-efAe-nmhZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Garcia - Run team-level cross validation"
      ],
      "metadata": {
        "id": "44Je521onVhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[plot_list, labels, binary_labels] = get_fold_data(fold_groups[test_fold], incv3=False, score_col=\"norm_task_score\")\n",
        "y_test_norm = labels\n",
        "\n",
        "[plot_list, labels, binary_labels] = get_fold_data(fold_groups[test_fold], incv3=False, score_col=\"task_score\")\n",
        "y_test = labels\n",
        "\n",
        "print(y_test)\n",
        "print(y_test_norm)\n"
      ],
      "metadata": {
        "id": "R2q6JhBIvb2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store metrics for all iterations\n",
        "maes = []    # MAE (mean absolute errors)\n",
        "mses = []    # MSE (mean squared errors)\n",
        "rmses = []   # RMSE (root mean squared errors)\n",
        "r2s = []     # r2 score\n",
        "corrs = []   # spearman correlations\n",
        "ps = []      # spearman correlation p-values\n",
        "\n",
        "all_y_test_task_score = [[] for i in range(num_iters)]\n",
        "predictions_task_score = [[] for i in range(num_iters)]\n",
        "predict_proba_task_score = [[] for i in range(num_iters)]\n",
        "\n",
        "# For each iteration \n",
        "for i in range(num_iters):\n",
        "    print(\"Iteration: \", i+1)\n",
        "    \n",
        "    # Lists for cumulative predictions for iteration\n",
        "    all_y_test = []\n",
        "    predictions = []\n",
        "    predict_proba = []\n",
        "    \n",
        "    # Get fold groups\n",
        "    fold_groups = folds_dict_list[i]\n",
        "    \n",
        "    # For each fold\n",
        "    for j, (test_fold, train_fold) in enumerate(zip(test_folds, train_folds)):\n",
        "        print(\"\\tFold: \", j+1)\n",
        "        # Get data for teams in test set\n",
        "        if normalize:\n",
        "          [plot_list, labels, binary_labels] = get_fold_data(fold_groups[test_fold], incv3=False, score_col=\"norm_task_score\")\n",
        "        else:\n",
        "          [plot_list, labels, binary_labels] = get_fold_data(fold_groups[test_fold], incv3=False, score_col=\"task_score\")\n",
        "        \n",
        "        X_test = plot_list\n",
        "        y_test = labels\n",
        "        all_y_test.extend(y_test.tolist())\n",
        "        \n",
        "        # Get data for teams in train set\n",
        "        if normalize:\n",
        "          [plot_list, labels, binary_labels] = get_fold_data(fold_groups[train_fold], incv3=False, score_col=\"norm_task_score\")\n",
        "        if normalize:\n",
        "          [plot_list, labels, binary_labels] = get_fold_data(fold_groups[train_fold], incv3=False, score_col=\"task_score\")\n",
        "        X_train = plot_list\n",
        "        y_train = binary_labels\n",
        "\n",
        "        # Train model\n",
        "        model_reg = get_reg_garcia_model()\n",
        "        # model_reg.summary()\n",
        "        info_reg_cv = model_reg.fit(x=X_train, y=y_train, epochs=num_epochs, verbose=False)\n",
        "\n",
        "        # Test model: https://androidkt.com/get-class-labels-from-predict-method-in-keras/\n",
        "        y_pp = model_reg.predict(X_test)\n",
        "\n",
        "        predict_proba.extend(y_pp.tolist())\n",
        "        # predictions.extend(y_pred.tolist())\n",
        "\n",
        "    # ----- END OF FOLDS\n",
        "\n",
        "    all_y_test = np.array(all_y_test)\n",
        "    all_y_test_task_score[i] = all_y_test\n",
        "\n",
        "    # predictions = np.squeeze(np.array(predictions))\n",
        "    predict_proba = np.squeeze(np.array(predict_proba))\n",
        "    \n",
        "    # predictions_task_score[i] = predictions\n",
        "    predict_proba_task_score[i] = predict_proba\n",
        "    \n",
        "    \n",
        "    # Calculate the evaluation metrics of the iteration\n",
        "    mae = mean_absolute_error(all_y_test, predict_proba)\n",
        "    mse = mean_squared_error(all_y_test, predict_proba)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(all_y_test, predict_proba)\n",
        "    corr, p = spearmanr(all_y_test, predict_proba)\n",
        "    \n",
        "    maes.append(mae)\n",
        "    mses.append(mse)\n",
        "    rmses.append(rmse)\n",
        "    r2s.append(r2)\n",
        "    corrs.append(corr)\n",
        "    ps.append(p)\n",
        "    \n",
        "    # Save actual labels and predictions for iteration\n",
        "    dfTruevPred = pd.DataFrame({'actual': all_y_test, 'predict_proba': predict_proba} #, 'prediction': predictions})\n",
        "\n",
        "    if shuffled:\n",
        "      ACTvPRED_SAVE_DIR = PROJECT_DIR + \"results/shuffled/cross_val/Regression/Garcia_model/\" + image_size + \"/\" + image_size + \"_TaskScore_True_vs_Pred_SHUFF_\" + str(i+1) + \".csv\"\n",
        "    else:\n",
        "      ACTvPRED_SAVE_DIR = PROJECT_DIR + \"results/cross_val/Regression/Garcia_model/\" + image_size + \"/\" + image_size + \"_TaskScore_True_vs_Pred_\" + str(i+1) + \".csv\"\n",
        "    dfTruevPred.to_csv(ACTvPRED_SAVE_DIR, index=False)    \n",
        "    \n",
        "# ----- END OF ITERATIONS\n",
        "\n",
        "print(\"\\n =========== ALL ITERATIONS RESULTS SUMMARY ===========\")\n",
        "dfMetrics = pd.DataFrame({'iteration': [i for i in range(1,num_iters+1)], \\\n",
        "                          'mae': maes, 'mse': mses, 'rmse': rmses, 'r2': r2s, 'corrs': corrs, 'ps': ps})\n",
        "\n",
        "display(dfMetrics)\n",
        "\n",
        "if shuffled:\n",
        "  METRICS_SAVE_DIR = PROJECT_DIR + \"results/shuffled/cross_val/Regression/Garcia_model/\" + image_size + \"/\" + image_size + \"_TaskScore_Metrics_SHUFF.csv\"\n",
        "else:\n",
        "  METRICS_SAVE_DIR = PROJECT_DIR + \"results/cross_val/Regression/Garcia_model/\" + image_size + \"/\" + image_size + \"_TaskScore_Metrics.csv\"\n",
        "dfMetrics.to_csv(METRICS_SAVE_DIR, index=False)   \n",
        "\n",
        "print(\"Averages over all iterations:\")\n",
        "print(\"%6s %.2f\" % (\"MAE:\", np.mean(dfMetrics['mae'])))\n",
        "print(\"%6s %.2f\" % (\"MSE:\", np.mean(dfMetrics['mse'])))\n",
        "print(\"%6s %.2f\" % (\"RMSE:\", np.mean(dfMetrics['rmse'])))\n",
        "print(\"%6s %.2f\" % (\"r2:\", np.mean(dfMetrics['r2'])))\n",
        "print(\"%6s %.2f\" % (\"Corr:\", np.mean(dfMetrics['corrs'])))\n",
        "print(\"%6s %.7f\" % (\"p-val:\", np.mean(dfMetrics['ps'])))\n",
        "\n",
        "print(\"\\n%6s %.2f\" % (\"Med corr:\", np.median(dfMetrics['corrs'])))\n",
        "\n",
        "# Get median iterations\n",
        "med_auroc = np.median(corrs)\n",
        "med_corr_idx = np.argsort(corrs)[len(corrs)//2]\n",
        "print(\"\\nMedian iteration number: \", med_corr_idx+1)\n",
        "\n",
        "# PLOT ACTUAL vs. PREDICTED \n",
        "dfPlot = pd.DataFrame({\"med_y_test\": all_y_test_task_score[med_corr_idx], \"med_predict_proba\": predict_proba_task_score[med_corr_idx]})\n",
        "dfPlot.plot(y=['med_y_test', 'med_predict_proba'], title='Actual vs. Predicted Task Score for Median Model', \\\n",
        "                  style=['b-', 'ro'], figsize=(20, 5))\n",
        "\n",
        "residuals = all_y_test_task_score[med_corr_idx] - predict_proba_task_score[med_corr_idx]\n",
        "sns.distplot(residuals)\n"
      ],
      "metadata": {
        "id": "YPB_BOmNnW1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Inception v3 model"
      ],
      "metadata": {
        "id": "IGZDDKVXm8bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reg_incv3_model():\n",
        "  # Use InceptionV3 model for feature extraction\n",
        "  ## Resource: http://marubon-ds.blogspot.com/2017/10/inceptionv3-fine-tuning-model.html\n",
        "\n",
        "  new_input = Input(shape=(imgsize[0], imgsize[1], 3)) \n",
        "\n",
        "  pretrained_model = tf.keras.applications.InceptionV3(\n",
        "                    include_top=False,\n",
        "                    weights=\"imagenet\",\n",
        "                    input_tensor=new_input\n",
        "  )\n",
        "\n",
        "  x = pretrained_model.output\n",
        "  x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "  out = layers.Dense(1, activation=\"linear\")(x)   # TODO: Is this the proper output for regression?\n",
        "\n",
        "\n",
        "  lr = 0.0001 # paper used 0.001\n",
        "  eps = 1e-08   # paper used 1e-08, but keras default is 1e-07\n",
        "  adam = keras.optimizers.Adam(learning_rate=lr, epsilon=eps)\n",
        "\n",
        "  mse = tf.keras.metrics.MeanSquaredError()\n",
        "  rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "  mae = tf.keras.metrics.MeanAbsoluteError()\n",
        "  cos_sim = tf.keras.metrics.CosineSimilarity(axis=1)\n",
        "\n",
        "  # TODO: change metrics for regression\n",
        "  model_pretrained = Model(inputs=new_input, outputs=out)\n",
        "  model_pretrained.compile(\n",
        "    optimizer=adam,\n",
        "    loss=\"mse\", \n",
        "    metrics=['accuracy', mse, rmse, mae, cos_sim]\n",
        "  )\n",
        "\n",
        "  return model_pretrained\n",
        "\n",
        "\n",
        "model_pretrained = get_reg_incv3_model()\n",
        "model_pretrained.summary()\n"
      ],
      "metadata": {
        "id": "qg8y5_8mm_5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}