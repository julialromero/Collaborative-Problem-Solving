{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a48addfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "import itertools\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot as plt\n",
    "import shap \n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8de4c",
   "metadata": {},
   "source": [
    "# Define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df414ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = False\n",
    "\n",
    "MEASURES_FILE = \"MdRQA_measures.csv\"\n",
    "SHUFF_MEASURES_FILE = \"shuff_MdRQA_measures.csv\"\n",
    "LABELS_FILE = \"team_block_outcomes.csv\"\n",
    "RESULTS = \"results/\" # \"results_no_RR/\" \n",
    "\n",
    "if normalize:\n",
    "    TASK_SCORE_RESULTS = RESULTS + \"task_score_norm/\"\n",
    "else:\n",
    "    TASK_SCORE_RESULTS = RESULTS + \"task_score/\"\n",
    "\n",
    "SUBJ_OUTCOME_RESULTS = RESULTS + \"subjective_outcome/\"\n",
    "VALENCE_RESULTS = RESULTS + \"valence/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084f123",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2951565c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MdRQA_measures.csv shape: (271, 11)\n",
      "shuff_MdRQA_measures.csv shape: (271, 11)\n",
      "team_block_outcomes.csv shape: (271, 7)\n"
     ]
    }
   ],
   "source": [
    "dfMeasures = pd.read_csv(MEASURES_FILE)\n",
    "dfShuffMeasures = pd.read_csv(SHUFF_MEASURES_FILE)\n",
    "dfLabels = pd.read_csv(LABELS_FILE)\n",
    "\n",
    "print(\"%s shape: %s\" % (MEASURES_FILE, dfMeasures.shape))\n",
    "print(\"%s shape: %s\" % (SHUFF_MEASURES_FILE, dfShuffMeasures.shape))\n",
    "print(\"%s shape: %s\" % (LABELS_FILE, dfLabels.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5bed641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['REC', 'DET', 'ADL', 'MDL', 'DENTR', 'LAM', 'AVL', 'MVL', 'VENTR'] #['REC', 'DET', 'ADL', 'MDL', 'DENTR', 'LAM', 'AVL', 'MVL', 'VENTR']\n",
    "\n",
    "\n",
    "def get_group_data(dfData, GROUPID_list, features_list, label_names):\n",
    "    dfGroupsData = pd.DataFrame()    \n",
    "    for GROUPID in GROUPID_list:\n",
    "        dfGroupsData = pd.concat([dfGroupsData, dfData.loc[dfData['GROUPID'] == GROUPID, :]], ignore_index=True)\n",
    "        \n",
    "    data = dfGroupsData.loc[:, features_list]  \n",
    "    labels = dfGroupsData.loc[:, label_names]\n",
    "\n",
    "    return [data, labels, dfGroupsData]\n",
    "\n",
    "def min_max_scaling(series):\n",
    "    # https://datagy.io/pandas-normalize-column/\n",
    "    return (series - series.min()) / (series.max() - series.min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7980252",
   "metadata": {},
   "source": [
    "# Experiment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9e0b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfData shape:  (271, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GROUPID</th>\n",
       "      <th>block</th>\n",
       "      <th>REC</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADL</th>\n",
       "      <th>MDL</th>\n",
       "      <th>DENTR</th>\n",
       "      <th>LAM</th>\n",
       "      <th>AVL</th>\n",
       "      <th>MVL</th>\n",
       "      <th>VENTR</th>\n",
       "      <th>CPS_and_ITN_mean</th>\n",
       "      <th>Valence</th>\n",
       "      <th>num_gold</th>\n",
       "      <th>num_silver</th>\n",
       "      <th>task_score</th>\n",
       "      <th>task_score_bin</th>\n",
       "      <th>norm_task_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010</td>\n",
       "      <td>ExpBlock1</td>\n",
       "      <td>0.111890</td>\n",
       "      <td>46.765002</td>\n",
       "      <td>2.580148</td>\n",
       "      <td>12</td>\n",
       "      <td>8.031479</td>\n",
       "      <td>63.476117</td>\n",
       "      <td>3.202765</td>\n",
       "      <td>8</td>\n",
       "      <td>8.276899</td>\n",
       "      <td>-0.304697</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010</td>\n",
       "      <td>ExpBlock2</td>\n",
       "      <td>0.221818</td>\n",
       "      <td>51.941830</td>\n",
       "      <td>2.801763</td>\n",
       "      <td>13</td>\n",
       "      <td>8.693110</td>\n",
       "      <td>71.437617</td>\n",
       "      <td>3.567544</td>\n",
       "      <td>14</td>\n",
       "      <td>8.801583</td>\n",
       "      <td>-0.304697</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10100</td>\n",
       "      <td>ExpBlock1</td>\n",
       "      <td>0.469005</td>\n",
       "      <td>68.603876</td>\n",
       "      <td>3.236574</td>\n",
       "      <td>27</td>\n",
       "      <td>9.502080</td>\n",
       "      <td>83.376781</td>\n",
       "      <td>4.480822</td>\n",
       "      <td>28</td>\n",
       "      <td>9.315484</td>\n",
       "      <td>0.486795</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10100</td>\n",
       "      <td>ExpBlock2</td>\n",
       "      <td>0.328632</td>\n",
       "      <td>63.319890</td>\n",
       "      <td>3.373809</td>\n",
       "      <td>34</td>\n",
       "      <td>8.995863</td>\n",
       "      <td>78.390610</td>\n",
       "      <td>4.336822</td>\n",
       "      <td>35</td>\n",
       "      <td>8.899755</td>\n",
       "      <td>0.545526</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10102</td>\n",
       "      <td>ExpBlock1</td>\n",
       "      <td>0.275806</td>\n",
       "      <td>40.010880</td>\n",
       "      <td>2.781595</td>\n",
       "      <td>16</td>\n",
       "      <td>8.650819</td>\n",
       "      <td>60.603101</td>\n",
       "      <td>3.351069</td>\n",
       "      <td>15</td>\n",
       "      <td>8.895810</td>\n",
       "      <td>0.454631</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2070</td>\n",
       "      <td>ExpBlock2</td>\n",
       "      <td>0.128570</td>\n",
       "      <td>35.135661</td>\n",
       "      <td>2.392715</td>\n",
       "      <td>9</td>\n",
       "      <td>7.967704</td>\n",
       "      <td>59.342180</td>\n",
       "      <td>2.886733</td>\n",
       "      <td>9</td>\n",
       "      <td>8.447403</td>\n",
       "      <td>0.302205</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2070</td>\n",
       "      <td>Warmup</td>\n",
       "      <td>0.411115</td>\n",
       "      <td>49.651775</td>\n",
       "      <td>2.839304</td>\n",
       "      <td>42</td>\n",
       "      <td>9.178902</td>\n",
       "      <td>70.710415</td>\n",
       "      <td>3.726037</td>\n",
       "      <td>43</td>\n",
       "      <td>9.143787</td>\n",
       "      <td>0.489594</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>2071</td>\n",
       "      <td>ExpBlock1</td>\n",
       "      <td>0.291773</td>\n",
       "      <td>45.826191</td>\n",
       "      <td>2.544979</td>\n",
       "      <td>11</td>\n",
       "      <td>8.971522</td>\n",
       "      <td>68.343464</td>\n",
       "      <td>3.244426</td>\n",
       "      <td>12</td>\n",
       "      <td>9.145826</td>\n",
       "      <td>0.032318</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>2071</td>\n",
       "      <td>ExpBlock2</td>\n",
       "      <td>0.188310</td>\n",
       "      <td>46.079278</td>\n",
       "      <td>2.572276</td>\n",
       "      <td>16</td>\n",
       "      <td>8.522221</td>\n",
       "      <td>66.710627</td>\n",
       "      <td>3.169010</td>\n",
       "      <td>17</td>\n",
       "      <td>8.746838</td>\n",
       "      <td>0.117616</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2071</td>\n",
       "      <td>Warmup</td>\n",
       "      <td>0.268505</td>\n",
       "      <td>43.939464</td>\n",
       "      <td>2.533835</td>\n",
       "      <td>10</td>\n",
       "      <td>8.857327</td>\n",
       "      <td>67.339723</td>\n",
       "      <td>3.216277</td>\n",
       "      <td>11</td>\n",
       "      <td>9.076562</td>\n",
       "      <td>-0.061376</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GROUPID      block       REC        DET       ADL  MDL     DENTR  \\\n",
       "0       1010  ExpBlock1  0.111890  46.765002  2.580148   12  8.031479   \n",
       "1       1010  ExpBlock2  0.221818  51.941830  2.801763   13  8.693110   \n",
       "2      10100  ExpBlock1  0.469005  68.603876  3.236574   27  9.502080   \n",
       "3      10100  ExpBlock2  0.328632  63.319890  3.373809   34  8.995863   \n",
       "4      10102  ExpBlock1  0.275806  40.010880  2.781595   16  8.650819   \n",
       "..       ...        ...       ...        ...       ...  ...       ...   \n",
       "266     2070  ExpBlock2  0.128570  35.135661  2.392715    9  7.967704   \n",
       "267     2070     Warmup  0.411115  49.651775  2.839304   42  9.178902   \n",
       "268     2071  ExpBlock1  0.291773  45.826191  2.544979   11  8.971522   \n",
       "269     2071  ExpBlock2  0.188310  46.079278  2.572276   16  8.522221   \n",
       "270     2071     Warmup  0.268505  43.939464  2.533835   10  8.857327   \n",
       "\n",
       "           LAM       AVL  MVL     VENTR  CPS_and_ITN_mean   Valence  num_gold  \\\n",
       "0    63.476117  3.202765    8  8.276899         -0.304697  3.666667         1   \n",
       "1    71.437617  3.567544   14  8.801583         -0.304697  3.666667         2   \n",
       "2    83.376781  4.480822   28  9.315484          0.486795  4.000000         0   \n",
       "3    78.390610  4.336822   35  8.899755          0.545526  4.666667         0   \n",
       "4    60.603101  3.351069   15  8.895810          0.454631  4.333333         3   \n",
       "..         ...       ...  ...       ...               ...       ...       ...   \n",
       "266  59.342180  2.886733    9  8.447403          0.302205  4.000000         0   \n",
       "267  70.710415  3.726037   43  9.143787          0.489594  4.000000         3   \n",
       "268  68.343464  3.244426   12  9.145826          0.032318  2.333333         0   \n",
       "269  66.710627  3.169010   17  8.746838          0.117616  4.333333         1   \n",
       "270  67.339723  3.216277   11  9.076562         -0.061376  4.000000         0   \n",
       "\n",
       "     num_silver  task_score  task_score_bin  norm_task_score  \n",
       "0             1           3               1         0.200000  \n",
       "1             1           5               1         0.333333  \n",
       "2             0           0               0         0.000000  \n",
       "3             3           3               1         0.200000  \n",
       "4             2           8               1         0.533333  \n",
       "..          ...         ...             ...              ...  \n",
       "266           4           4               1         0.266667  \n",
       "267           0           6               1         0.400000  \n",
       "268           0           0               0         0.000000  \n",
       "269           4           6               1         0.400000  \n",
       "270           2           2               0         0.133333  \n",
       "\n",
       "[271 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get model type\n",
    "shuffled = False\n",
    "chance = False\n",
    "num_iters = 25\n",
    "num_folds = 10\n",
    "model = \"RFR\"\n",
    "\n",
    "if shuffled:\n",
    "    dfData = pd.merge(dfShuffMeasures, dfLabels, on=['GROUPID', 'block'], how='inner')\n",
    "    # Drop rows with NaN for ADL or AVL\n",
    "    dfData = dfData.dropna()\n",
    "elif chance:\n",
    "    dfData = pd.merge(dfMeasures, dfLabels, on=['GROUPID', 'block'], how='inner')\n",
    "    # Shuffle labels\n",
    "    label_cols = ['CPS_and_ITN_mean', 'Valence', 'num_gold', 'num_silver', 'task_score']\n",
    "    dfData.loc[:, label_cols] = shuffle(dfData.loc[:, label_cols], random_state=12).reset_index(drop=True)\n",
    "else:\n",
    "    dfData = pd.merge(dfMeasures, dfLabels, on=['GROUPID', 'block'], how='inner')\n",
    "\n",
    "# Add on binary task score\n",
    "median = dfData['task_score'].median()\n",
    "dfData['task_score_bin'] = np.where(dfData['task_score'] <= median, 0, 1)\n",
    "\n",
    "# Add on normalized task score\n",
    "dfData['norm_task_score'] = min_max_scaling(dfData['task_score'])\n",
    "\n",
    "\n",
    "print(\"dfData shape: \", dfData.shape)\n",
    "display(dfData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0743a",
   "metadata": {},
   "source": [
    "## Prep for team-level cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "930a0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "* No team overlap *\n",
      "Iteration:  2\n",
      "* No team overlap *\n",
      "Iteration:  3\n",
      "* No team overlap *\n",
      "Iteration:  4\n",
      "* No team overlap *\n",
      "Iteration:  5\n",
      "* No team overlap *\n",
      "Iteration:  6\n",
      "* No team overlap *\n",
      "Iteration:  7\n",
      "* No team overlap *\n",
      "Iteration:  8\n",
      "* No team overlap *\n",
      "Iteration:  9\n",
      "* No team overlap *\n",
      "Iteration:  10\n",
      "* No team overlap *\n",
      "Iteration:  11\n",
      "* No team overlap *\n",
      "Iteration:  12\n",
      "* No team overlap *\n",
      "Iteration:  13\n",
      "* No team overlap *\n",
      "Iteration:  14\n",
      "* No team overlap *\n",
      "Iteration:  15\n",
      "* No team overlap *\n",
      "Iteration:  16\n",
      "* No team overlap *\n",
      "Iteration:  17\n",
      "* No team overlap *\n",
      "Iteration:  18\n",
      "* No team overlap *\n",
      "Iteration:  19\n",
      "* No team overlap *\n",
      "Iteration:  20\n",
      "* No team overlap *\n",
      "Iteration:  21\n",
      "* No team overlap *\n",
      "Iteration:  22\n",
      "* No team overlap *\n",
      "Iteration:  23\n",
      "* No team overlap *\n",
      "Iteration:  24\n",
      "* No team overlap *\n",
      "Iteration:  25\n",
      "* No team overlap *\n",
      "\n",
      "Number of iterations:  25\n",
      "\n",
      "Iterating through folds_dict_list to check for overlap...\n",
      "* No team overlap *\n"
     ]
    }
   ],
   "source": [
    "# Define fold names\n",
    "train_folds = []\n",
    "test_folds = []\n",
    "set_type = \"test\"\n",
    "for j in range(1,num_folds+1): \n",
    "    col_name = \"Fold\" + str(j) + \"_\" + set_type\n",
    "    test_folds.append(col_name)\n",
    "    set_type = \"train\"  \n",
    "    col_name = \"Fold\" + str(j) + \"_\" + set_type\n",
    "    train_folds.append(col_name)\n",
    "    set_type = \"test\"\n",
    "\n",
    "folds_dict_list = []\n",
    "\n",
    "# Split teams into 5 groups # TODO: Need to use dfLabels to match the same folds as the CNN models\n",
    "teams = pd.unique(dfMeasures.GROUPID)\n",
    "\n",
    "# For every iteration\n",
    "for i in range(1,num_iters+1):\n",
    "    print(\"Iteration: \", i)\n",
    "    teams = shuffle(teams, random_state=i)\n",
    "    groups = np.array_split(teams, num_folds)\n",
    "    \n",
    "    # Define groups for each fold\n",
    "    fold_groups = {}\n",
    "    for j, (train_fold, test_fold) in enumerate(zip(train_folds, test_folds)):\n",
    "        # make the current group the test group\n",
    "        fold_groups[test_fold] = groups[j]\n",
    "        # make all other groups the train group\n",
    "        train_group = groups[:j] + groups[j+1:]\n",
    "        train_group = [team for group in train_group for team in group]\n",
    "        fold_groups[train_fold] = train_group\n",
    "        \n",
    "    ## Confirm that for each fold, there is no team overlap bewteen train and test set\n",
    "    for j in range(1,num_folds+1):\n",
    "        assert set(fold_groups['Fold'+str(j)+'_test']).isdisjoint(set(fold_groups['Fold'+str(j)+'_train'])), \"There is overlap in train and test set \" + str(j)\n",
    "    \n",
    "    print(\"* No team overlap *\")\n",
    "\n",
    "      \n",
    "    # Add fold groups to dictionary\n",
    "    folds_dict_list.append(fold_groups)\n",
    "    \n",
    "\n",
    "# Informational\n",
    "print(\"\\nNumber of iterations: \", len(folds_dict_list))\n",
    "\n",
    "print(\"\\nIterating through folds_dict_list to check for overlap...\")\n",
    "for i,dicti in enumerate(folds_dict_list):\n",
    "    for j in range(1,num_folds+1):\n",
    "        assert set(dicti['Fold'+str(j)+'_test']).isdisjoint(set(dicti['Fold'+str(j)+'_train'])), \"There is overlap in train and test set \" + str(j)\n",
    "    \n",
    "print(\"* No team overlap *\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a648fe",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68389051",
   "metadata": {},
   "source": [
    "## Predict Task Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "745c66e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n",
      "Iteration:  15\n",
      "Iteration:  16\n",
      "Iteration:  17\n",
      "Iteration:  18\n",
      "Iteration:  19\n",
      "Iteration:  20\n",
      "Iteration:  21\n",
      "Iteration:  22\n",
      "Iteration:  23\n",
      "Iteration:  24\n",
      "Iteration:  25\n",
      "\n",
      " =========== ALL ITERATIONS RESULTS SUMMARY ===========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>corrs</th>\n",
       "      <th>ps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.579742</td>\n",
       "      <td>11.066096</td>\n",
       "      <td>3.326574</td>\n",
       "      <td>0.334312</td>\n",
       "      <td>1.689014e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.614945</td>\n",
       "      <td>11.344875</td>\n",
       "      <td>3.368215</td>\n",
       "      <td>0.324717</td>\n",
       "      <td>4.506974e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.625166</td>\n",
       "      <td>11.252868</td>\n",
       "      <td>3.354529</td>\n",
       "      <td>0.320615</td>\n",
       "      <td>6.786916e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.558339</td>\n",
       "      <td>10.993112</td>\n",
       "      <td>3.315586</td>\n",
       "      <td>0.347191</td>\n",
       "      <td>4.285814e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.603321</td>\n",
       "      <td>11.406511</td>\n",
       "      <td>3.377353</td>\n",
       "      <td>0.343481</td>\n",
       "      <td>6.403510e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2.621476</td>\n",
       "      <td>11.433815</td>\n",
       "      <td>3.381393</td>\n",
       "      <td>0.320041</td>\n",
       "      <td>7.183319e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2.582214</td>\n",
       "      <td>11.129983</td>\n",
       "      <td>3.336163</td>\n",
       "      <td>0.346721</td>\n",
       "      <td>4.510882e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2.626494</td>\n",
       "      <td>11.697700</td>\n",
       "      <td>3.420190</td>\n",
       "      <td>0.310086</td>\n",
       "      <td>1.887857e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2.617085</td>\n",
       "      <td>11.408055</td>\n",
       "      <td>3.377581</td>\n",
       "      <td>0.326068</td>\n",
       "      <td>3.933127e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2.611587</td>\n",
       "      <td>11.476111</td>\n",
       "      <td>3.387641</td>\n",
       "      <td>0.312920</td>\n",
       "      <td>1.439053e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>2.565129</td>\n",
       "      <td>10.871280</td>\n",
       "      <td>3.297162</td>\n",
       "      <td>0.361327</td>\n",
       "      <td>8.843067e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>2.636199</td>\n",
       "      <td>11.515052</td>\n",
       "      <td>3.393384</td>\n",
       "      <td>0.309159</td>\n",
       "      <td>2.061942e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2.629336</td>\n",
       "      <td>11.591885</td>\n",
       "      <td>3.404686</td>\n",
       "      <td>0.331749</td>\n",
       "      <td>2.202427e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>2.595572</td>\n",
       "      <td>11.176373</td>\n",
       "      <td>3.343108</td>\n",
       "      <td>0.331819</td>\n",
       "      <td>2.186663e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2.606679</td>\n",
       "      <td>11.328544</td>\n",
       "      <td>3.365790</td>\n",
       "      <td>0.328579</td>\n",
       "      <td>3.048294e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>2.614280</td>\n",
       "      <td>11.286222</td>\n",
       "      <td>3.359497</td>\n",
       "      <td>0.329351</td>\n",
       "      <td>2.817493e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>2.558487</td>\n",
       "      <td>11.012256</td>\n",
       "      <td>3.318472</td>\n",
       "      <td>0.356633</td>\n",
       "      <td>1.506451e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>2.612620</td>\n",
       "      <td>11.293985</td>\n",
       "      <td>3.360652</td>\n",
       "      <td>0.339147</td>\n",
       "      <td>1.016756e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>2.620295</td>\n",
       "      <td>11.434082</td>\n",
       "      <td>3.381432</td>\n",
       "      <td>0.316626</td>\n",
       "      <td>1.004691e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>2.606199</td>\n",
       "      <td>11.130569</td>\n",
       "      <td>3.336251</td>\n",
       "      <td>0.316908</td>\n",
       "      <td>9.773608e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>2.594834</td>\n",
       "      <td>10.891027</td>\n",
       "      <td>3.300156</td>\n",
       "      <td>0.338565</td>\n",
       "      <td>1.081339e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2.587122</td>\n",
       "      <td>11.017382</td>\n",
       "      <td>3.319244</td>\n",
       "      <td>0.341961</td>\n",
       "      <td>7.536344e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2.634797</td>\n",
       "      <td>11.497471</td>\n",
       "      <td>3.390792</td>\n",
       "      <td>0.315447</td>\n",
       "      <td>1.126881e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>2.602546</td>\n",
       "      <td>11.399642</td>\n",
       "      <td>3.376336</td>\n",
       "      <td>0.339354</td>\n",
       "      <td>9.946849e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2.644576</td>\n",
       "      <td>11.519018</td>\n",
       "      <td>3.393968</td>\n",
       "      <td>0.327056</td>\n",
       "      <td>3.559025e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration       mae        mse      rmse     corrs            ps\n",
       "0           1  2.579742  11.066096  3.326574  0.334312  1.689014e-08\n",
       "1           2  2.614945  11.344875  3.368215  0.324717  4.506974e-08\n",
       "2           3  2.625166  11.252868  3.354529  0.320615  6.786916e-08\n",
       "3           4  2.558339  10.993112  3.315586  0.347191  4.285814e-09\n",
       "4           5  2.603321  11.406511  3.377353  0.343481  6.403510e-09\n",
       "5           6  2.621476  11.433815  3.381393  0.320041  7.183319e-08\n",
       "6           7  2.582214  11.129983  3.336163  0.346721  4.510882e-09\n",
       "7           8  2.626494  11.697700  3.420190  0.310086  1.887857e-07\n",
       "8           9  2.617085  11.408055  3.377581  0.326068  3.933127e-08\n",
       "9          10  2.611587  11.476111  3.387641  0.312920  1.439053e-07\n",
       "10         11  2.565129  10.871280  3.297162  0.361327  8.843067e-10\n",
       "11         12  2.636199  11.515052  3.393384  0.309159  2.061942e-07\n",
       "12         13  2.629336  11.591885  3.404686  0.331749  2.202427e-08\n",
       "13         14  2.595572  11.176373  3.343108  0.331819  2.186663e-08\n",
       "14         15  2.606679  11.328544  3.365790  0.328579  3.048294e-08\n",
       "15         16  2.614280  11.286222  3.359497  0.329351  2.817493e-08\n",
       "16         17  2.558487  11.012256  3.318472  0.356633  1.506451e-09\n",
       "17         18  2.612620  11.293985  3.360652  0.339147  1.016756e-08\n",
       "18         19  2.620295  11.434082  3.381432  0.316626  1.004691e-07\n",
       "19         20  2.606199  11.130569  3.336251  0.316908  9.773608e-08\n",
       "20         21  2.594834  10.891027  3.300156  0.338565  1.081339e-08\n",
       "21         22  2.587122  11.017382  3.319244  0.341961  7.536344e-09\n",
       "22         23  2.634797  11.497471  3.390792  0.315447  1.126881e-07\n",
       "23         24  2.602546  11.399642  3.376336  0.339354  9.946849e-09\n",
       "24         25  2.644576  11.519018  3.393968  0.327056  3.559025e-08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average over all iterations:\n",
      "  MAE: 2.61\n",
      "  MSE: 11.29\n",
      " RMSE: 3.36\n",
      " Corr: 0.33\n",
      "p-val: 0.0000001\n",
      "\n",
      "Median iteration number:  16\n"
     ]
    }
   ],
   "source": [
    "# Resources\n",
    "## https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "\n",
    "# Store metrics for all iterations\n",
    "maes = []    # MAE (mean absolute errors)\n",
    "mses = []    # MSE (mean squared errors)\n",
    "rmses = []   # RMSE (root mean squared errors)\n",
    "corrs = []   # spearman correlations\n",
    "ps = []      # spearman correlation p-values\n",
    "\n",
    "all_y_test_task_score = [[] for i in range(num_iters)]\n",
    "predictions_task_score = [[] for i in range(num_iters)]\n",
    "predict_proba_task_score = [[] for i in range(num_iters)]\n",
    "\n",
    "# # For storing shap values across all folds\n",
    "# task_score_shap_values_0 = None\n",
    "# task_score_shap_values_1 = None\n",
    "# task_score_full_X_test = pd.DataFrame()\n",
    "\n",
    "#-----------------------------------------------#\n",
    "#      5-fold team level cross-validation       #\n",
    "#-----------------------------------------------#\n",
    "\n",
    "# For each iteration \n",
    "for i in range(num_iters):\n",
    "    print(\"Iteration: \", i+1)\n",
    "    \n",
    "    # Lists for cumulative test set and predictions for iteration\n",
    "    dfFullTest = pd.DataFrame()\n",
    "    all_y_test = []\n",
    "    predictions = []\n",
    "    imp_list = []\n",
    "    \n",
    "    # Create model for task_score prediction\n",
    "    if model == \"RFR\":\n",
    "        model_task_score = RandomForestRegressor(n_estimators=100, random_state=1, max_features='sqrt') \n",
    "\n",
    "    elif model == \"SVR\":\n",
    "        model_task_score = SVR()\n",
    "        \n",
    "    elif model == \"LR\":\n",
    "        model_task_score = LinearRegression()\n",
    "    \n",
    "    \n",
    "    # Get fold groups\n",
    "    fold_groups = folds_dict_list[i]\n",
    "    \n",
    "    # For each fold\n",
    "    for j, (test_fold, train_fold) in enumerate(zip(test_folds, train_folds)):\n",
    "#         print(\"\\tFold: \", j+1)\n",
    "        # Get data for teams in test set\n",
    "        if normalize:\n",
    "            test_data_list = get_group_data(dfData, fold_groups[test_fold], features, 'norm_task_score')\n",
    "        else:\n",
    "            test_data_list = get_group_data(dfData, fold_groups[test_fold], features, 'task_score')\n",
    "        X_test = test_data_list[0]\n",
    "        y_test = test_data_list[1]\n",
    "        all_y_test.extend(y_test.tolist())\n",
    "\n",
    "        dfFullTest = pd.concat([dfFullTest, test_data_list[2]], ignore_index=True)      \n",
    "        \n",
    "        # Get data for teams in train set\n",
    "        if normalize:\n",
    "            train_data_list = get_group_data(dfData, fold_groups[train_fold], features, 'norm_task_score')\n",
    "        else:\n",
    "            train_data_list = get_group_data(dfData, fold_groups[train_fold], features, 'task_score')\n",
    "        X_train = train_data_list[0]\n",
    "        y_train = train_data_list[1]\n",
    "\n",
    "        # Train model\n",
    "        model_task_score.fit(X_train, y_train)\n",
    "\n",
    "        # Test model\n",
    "        y_pred = model_task_score.predict(X_test)\n",
    "        predictions.extend(y_pred.tolist())\n",
    "        \n",
    "#         ## Get SHAP values\n",
    "#         explainer = shap.TreeExplainer(model_task_scorerfc_task_score)\n",
    "#         shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "#         if j==0:\n",
    "#             task_score_shap_values_0 = shap_values[0]\n",
    "#             task_score_shap_values_1 = shap_values[1]\n",
    "#         else:\n",
    "#             task_score_shap_values_0 = np.vstack([task_score_shap_values_0, shap_values[0]])\n",
    "#             task_score_shap_values_1 = np.vstack([task_score_shap_values_1, shap_values[1]])\n",
    "#         task_score_full_X_test = pd.concat([task_score_full_X_test, X_test], ignore_index=True)\n",
    "#         ## End of get SHAP values\n",
    "\n",
    "        if model == \"RFR\":    \n",
    "            ### Compute Feature Importances for last fold iteration ###\n",
    "            # Impurity-based importances\n",
    "            importances = model_task_score.feature_importances_\n",
    "            std = np.std([tree.feature_importances_ for tree in model_task_score.estimators_], axis=0)\n",
    "            imps = pd.Series(importances, index=features)\n",
    "            stds = pd.Series(std, index=features)\n",
    "            imp_list.append(imps)\n",
    "#             dfIterFeatureImportances = pd.concat([dfIterFeatureImportances, imps])\n",
    "            \n",
    "            \n",
    "#             ## Plot feature importances\n",
    "#             fig1, ax1 = plt.subplots()\n",
    "#             imps.plot.bar(yerr=std, ax=ax1)\n",
    "#             ax1.set_title(\"Task Score Feature importances using MDI\")\n",
    "#             ax1.set_ylabel(\"Mean decrease in impurity\")\n",
    "#             plt.show()\n",
    "#             ## END plotting feature importances\n",
    "\n",
    "# ----- END OF FOLDS\n",
    "\n",
    "    # Save feature importances for iteration\n",
    "    dfIterFeatureImportances = pd.DataFrame(imp_list, columns=features)\n",
    "\n",
    "    if shuffled:\n",
    "        dfIterFeatureImportances.to_csv(TASK_SCORE_RESULTS + \"RAW_SHUFFLED/\" + model + \"/feature_importances/\" + model + \"_feature_importances_SHUFF_\" + str(i+1) + \".csv\", index=False)\n",
    "    elif chance:\n",
    "        dfIterFeatureImportances.to_csv(TASK_SCORE_RESULTS + \"RAW_CHANCE/\" + model + \"/feature_importances/\" + model + \"_feature_importances_CHANCE_\" + str(i+1) + \".csv\", index=False)\n",
    "    else:\n",
    "        dfIterFeatureImportances.to_csv(TASK_SCORE_RESULTS + \"RAW/\" + model + \"/feature_importances/\" + model + \"_feature_importances_\" + str(i+1) + \".csv\", index=False)\n",
    "    \n",
    "    \n",
    "    # Save metrics for iteration\n",
    "    all_y_test = np.array(all_y_test)\n",
    "    all_y_test_task_score[i] = all_y_test\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    predictions_task_score[i] = predictions\n",
    "\n",
    "    # Calculate the absolute errors (MAE) of the iteration\n",
    "    mae = metrics.mean_absolute_error(all_y_test, predictions)\n",
    "    mse = metrics.mean_squared_error(all_y_test, predictions)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(all_y_test, predictions))\n",
    "    corr, p = spearmanr(all_y_test, predictions)\n",
    "    maes.append(mae)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    corrs.append(corr)\n",
    "    ps.append(p)\n",
    "\n",
    "     # Save actual labels and predictions for iteration\n",
    "    dfTruevPred = dfFullTest.loc[:, ['GROUPID', 'block', 'task_score']]\n",
    "    dfTruevPred['prediction'] = predictions\n",
    "    dfTruevPred.sort_values(['GROUPID', 'block', 'task_score'], ignore_index=True, inplace=True)\n",
    "\n",
    "    dfTruevPred['error'] = abs(dfTruevPred['prediction'] - dfTruevPred['task_score'])\n",
    "#     print(\"MAE from df: \", round(np.mean(dfTruevPred['error']), 2))\n",
    "#     # FOR PLOTTING ACTUAL vs. PREDICTED   \n",
    "#     dfTruevPred.plot(y=['task_score', 'prediction'], title='Actual vs. Predicted Task Score', \\\n",
    "#                      style=['b-', 'ro'], figsize=(20, 5))\n",
    "#     # END PLOTTING\n",
    "        \n",
    "    \n",
    "#     if shuffled:\n",
    "#         dfTruevPred.to_csv(TASK_SCORE_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_TaskScore_True_vs_Pred_SHUFF_\" + str(i+1) + \".csv\", index=False)\n",
    "#     elif chance:\n",
    "#         dfTruevPred.to_csv(TASK_SCORE_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_TaskScore_True_vs_Pred_CHANCE_\" + str(i+1) + \".csv\", index=False)\n",
    "#     else:\n",
    "#         dfTruevPred.to_csv(TASK_SCORE_RESULTS + \"RAW/\" + model + \"/\" + model + \"_TaskScore_True_vs_Pred_\" + str(i+1) + \".csv\", index=False)\n",
    "    \n",
    "    \n",
    "# ----- END OF ITERATIONS\n",
    "\n",
    "print(\"\\n =========== ALL ITERATIONS RESULTS SUMMARY ===========\")\n",
    "dfMetrics = pd.DataFrame({'iteration': [i for i in range(1,num_iters+1)], \\\n",
    "                          'mae': maes, 'mse': mses, 'rmse': rmses, 'corrs': corrs, 'ps': ps})\n",
    "\n",
    "display(dfMetrics)\n",
    "\n",
    "# if shuffled:\n",
    "#     dfMetrics.to_csv(TASK_SCORE_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_TaskScore_Metrics_SHUFF.csv\", index=False)\n",
    "# elif chance:\n",
    "#     dfMetrics.to_csv(TASK_SCORE_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_TaskScore_Metrics_CHANCE.csv\", index=False)\n",
    "# else:\n",
    "#     dfMetrics.to_csv(TASK_SCORE_RESULTS + \"RAW/\" + model + \"/\" + model + \"_TaskScore_Metrics.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Average over all iterations:\")\n",
    "print(\"%6s %.2f\" % (\"MAE:\", np.mean(dfMetrics['mae'])))\n",
    "print(\"%6s %.2f\" % (\"MSE:\", np.mean(dfMetrics['mse'])))\n",
    "print(\"%6s %.2f\" % (\"RMSE:\", np.mean(dfMetrics['rmse'])))\n",
    "print(\"%6s %.2f\" % (\"Corr:\", np.mean(dfMetrics['corrs'])))\n",
    "print(\"%6s %.7f\" % (\"p-val:\", np.mean(dfMetrics['ps'])))\n",
    "\n",
    "# Get median iterations\n",
    "med_corr_idx = np.argsort(corrs)[len(corrs)//2]\n",
    "print(\"\\nMedian iteration number: \", med_corr_idx+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b9b8e",
   "metadata": {},
   "source": [
    "## (LR ONLY) Get linear regression coefficients for the median model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e79e50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median iteration number:  6\n",
      "\tFold:  1\n",
      "\tFold:  2\n",
      "\tFold:  3\n",
      "\tFold:  4\n",
      "\tFold:  5\n",
      "\tFold:  6\n",
      "\tFold:  7\n",
      "\tFold:  8\n",
      "\tFold:  9\n",
      "\tFold:  10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>corrs</th>\n",
       "      <th>ps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.916833</td>\n",
       "      <td>12.460025</td>\n",
       "      <td>3.529876</td>\n",
       "      <td>-0.122763</td>\n",
       "      <td>0.525802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.713845</td>\n",
       "      <td>11.907378</td>\n",
       "      <td>3.450707</td>\n",
       "      <td>-0.044513</td>\n",
       "      <td>0.818653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.000780</td>\n",
       "      <td>12.818883</td>\n",
       "      <td>3.580347</td>\n",
       "      <td>0.034082</td>\n",
       "      <td>0.858101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.057397</td>\n",
       "      <td>13.492748</td>\n",
       "      <td>3.673248</td>\n",
       "      <td>0.135815</td>\n",
       "      <td>0.499382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.231021</td>\n",
       "      <td>6.437665</td>\n",
       "      <td>2.537255</td>\n",
       "      <td>-0.080026</td>\n",
       "      <td>0.697564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2.869123</td>\n",
       "      <td>13.556508</td>\n",
       "      <td>3.681916</td>\n",
       "      <td>0.348407</td>\n",
       "      <td>0.081108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2.412263</td>\n",
       "      <td>8.390828</td>\n",
       "      <td>2.896693</td>\n",
       "      <td>-0.203891</td>\n",
       "      <td>0.328281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>3.229915</td>\n",
       "      <td>18.392158</td>\n",
       "      <td>4.288608</td>\n",
       "      <td>-0.039042</td>\n",
       "      <td>0.846692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2.586288</td>\n",
       "      <td>10.148909</td>\n",
       "      <td>3.185735</td>\n",
       "      <td>-0.032661</td>\n",
       "      <td>0.876832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>3.242146</td>\n",
       "      <td>14.438409</td>\n",
       "      <td>3.799791</td>\n",
       "      <td>0.133655</td>\n",
       "      <td>0.506293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold       mae        mse      rmse     corrs        ps\n",
       "0     1  2.916833  12.460025  3.529876 -0.122763  0.525802\n",
       "1     2  2.713845  11.907378  3.450707 -0.044513  0.818653\n",
       "2     3  3.000780  12.818883  3.580347  0.034082  0.858101\n",
       "3     4  3.057397  13.492748  3.673248  0.135815  0.499382\n",
       "4     5  2.231021   6.437665  2.537255 -0.080026  0.697564\n",
       "5     6  2.869123  13.556508  3.681916  0.348407  0.081108\n",
       "6     7  2.412263   8.390828  2.896693 -0.203891  0.328281\n",
       "7     8  3.229915  18.392158  4.288608 -0.039042  0.846692\n",
       "8     9  2.586288  10.148909  3.185735 -0.032661  0.876832\n",
       "9    10  3.242146  14.438409  3.799791  0.133655  0.506293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average over all folds:\n",
      "  MAE: 2.83\n",
      "  MSE: 12.20\n",
      " RMSE: 3.46\n",
      " Corr: 0.01\n",
      "p-val: 0.6038708\n",
      "\n",
      "Median fold number:  9\n",
      "\n",
      "\n",
      "***** Now running median fold ......\n",
      "\tFold:  9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.122843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADL</th>\n",
       "      <td>2.052089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MDL</th>\n",
       "      <td>-0.573609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENTR</th>\n",
       "      <td>8.340108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAM</th>\n",
       "      <td>-0.259254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVL</th>\n",
       "      <td>-2.464830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MVL</th>\n",
       "      <td>0.556438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VENTR</th>\n",
       "      <td>-7.613451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Coefficient\n",
       "DET       0.122843\n",
       "ADL       2.052089\n",
       "MDL      -0.573609\n",
       "DENTR     8.340108\n",
       "LAM      -0.259254\n",
       "AVL      -2.464830\n",
       "MVL       0.556438\n",
       "VENTR    -7.613451"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "if model == \"LR\":\n",
    "    \n",
    "    i = med_corr_idx # for iteration number 1 (median iteration)\n",
    "    print(\"Median iteration number: \", i+1)\n",
    "    \n",
    "    # Store metrics for all iterations\n",
    "    maes = []    # MAE (mean absolute errors)\n",
    "    mses = []    # MSE (mean squared errors)\n",
    "    rmses = []   # RMSE (root mean squared errors)\n",
    "    corrs = []   # spearman correlations\n",
    "    ps = []      # spearman correlation p-values\n",
    "    \n",
    "    # Get fold groups\n",
    "    fold_groups = folds_dict_list[i]\n",
    "    \n",
    "    # For each fold\n",
    "    for j, (test_fold, train_fold) in enumerate(zip(test_folds, train_folds)):\n",
    "        print(\"\\tFold: \", j+1)\n",
    "        # Get data for teams in test set\n",
    "        if normalize:\n",
    "            test_data_list = get_group_data(dfData, fold_groups[test_fold], features, 'norm_task_score')\n",
    "        else:\n",
    "            test_data_list = get_group_data(dfData, fold_groups[test_fold], features, 'task_score')\n",
    "        X_test = test_data_list[0]\n",
    "        y_test = test_data_list[1]      \n",
    "        \n",
    "        # Get data for teams in train set\n",
    "        if normalize:\n",
    "            train_data_list = get_group_data(dfData, fold_groups[train_fold], features, 'norm_task_score')\n",
    "        else:\n",
    "            train_data_list = get_group_data(dfData, fold_groups[train_fold], features, 'task_score')\n",
    "        X_train = train_data_list[0]\n",
    "        y_train = train_data_list[1]\n",
    "\n",
    "        # Train model\n",
    "        model_task_score.fit(X_train, y_train)\n",
    "\n",
    "#         coeff_df = pd.DataFrame(model_task_score.coef_, X_train.columns, columns=['Coefficient'])\n",
    "#         display(coeff_df)\n",
    "        \n",
    "        # Test model\n",
    "        y_pred = model_task_score.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics of the fold\n",
    "        mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "        mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "        corr, p = spearmanr(y_test, y_pred)\n",
    "        maes.append(mae)\n",
    "        mses.append(mse)\n",
    "        rmses.append(rmse)\n",
    "        corrs.append(corr)\n",
    "        ps.append(p)\n",
    "        \n",
    "    \n",
    "    dfMetrics = pd.DataFrame({'fold': [i for i in range(1,len(test_folds)+1)], \\\n",
    "                          'mae': maes, 'mse': mses, 'rmse': rmses, 'corrs': corrs, 'ps': ps})\n",
    "    \n",
    "    display(dfMetrics)\n",
    "    \n",
    "    \n",
    "    print(\"Average over all folds:\")\n",
    "    print(\"%6s %.2f\" % (\"MAE:\", np.mean(dfMetrics['mae'])))\n",
    "    print(\"%6s %.2f\" % (\"MSE:\", np.mean(dfMetrics['mse'])))\n",
    "    print(\"%6s %.2f\" % (\"RMSE:\", np.mean(dfMetrics['rmse'])))\n",
    "    print(\"%6s %.2f\" % (\"Corr:\", np.mean(dfMetrics['corrs'])))\n",
    "    print(\"%6s %.7f\" % (\"p-val:\", np.mean(dfMetrics['ps'])))\n",
    "\n",
    "    # Get median fold\n",
    "    med_corr_idx = np.argsort(corrs)[len(corrs)//2]\n",
    "    print(\"\\nMedian fold number: \", med_corr_idx+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"\\n\\n***** Now running median fold ......\")\n",
    "\n",
    "fold_idx = med_corr_idx\n",
    "test_fold = test_folds[fold_idx]\n",
    "train_fold = train_folds[fold_idx]\n",
    "\n",
    "print(\"\\tFold: \", fold_idx+1)\n",
    "\n",
    "# Get data for teams in test set\n",
    "test_data_list = get_group_data(dfData, fold_groups[test_fold], features, 'task_score')\n",
    "X_test = test_data_list[0]\n",
    "y_test = test_data_list[1]      \n",
    "\n",
    "# Get data for teams in train set\n",
    "train_data_list = get_group_data(dfData, fold_groups[train_fold], features, 'task_score')\n",
    "X_train = train_data_list[0]\n",
    "y_train = train_data_list[1]\n",
    "\n",
    "# Train model\n",
    "model_task_score.fit(X_train, y_train)\n",
    "\n",
    "coeff_df = pd.DataFrame(model_task_score.coef_, X_train.columns, columns=['Coefficient'])\n",
    "display(coeff_df)\n",
    "\n",
    "\n",
    "if shuffled:\n",
    "    coeff_df.to_csv(TASK_SCORE_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_Median_iter_fold_LR_coeffs_SHUFF.csv\")\n",
    "elif chance:\n",
    "    coeff_df.to_csv(TASK_SCORE_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_Median_iter_fold_LR_coeffs_CHANCE.csv\")\n",
    "else:\n",
    "    coeff_df.to_csv(TASK_SCORE_RESULTS + \"RAW/\" + model + \"/\" + model + \"_Median_iter_fold_LR_coeffs.csv\")\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe617ac5",
   "metadata": {},
   "source": [
    "# Predict Subjective Outcome (combined CPS and ITN score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1272b86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n",
      "Iteration:  15\n",
      "Iteration:  16\n",
      "Iteration:  17\n",
      "Iteration:  18\n",
      "Iteration:  19\n",
      "Iteration:  20\n",
      "Iteration:  21\n",
      "Iteration:  22\n",
      "Iteration:  23\n",
      "Iteration:  24\n",
      "Iteration:  25\n",
      "\n",
      " =========== ALL ITERATIONS RESULTS SUMMARY ===========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>corrs</th>\n",
       "      <th>ps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.430072</td>\n",
       "      <td>0.301220</td>\n",
       "      <td>0.548835</td>\n",
       "      <td>0.033603</td>\n",
       "      <td>0.581785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.427457</td>\n",
       "      <td>0.294980</td>\n",
       "      <td>0.543121</td>\n",
       "      <td>0.081537</td>\n",
       "      <td>0.180802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.438217</td>\n",
       "      <td>0.309540</td>\n",
       "      <td>0.556364</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.823838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.432567</td>\n",
       "      <td>0.298368</td>\n",
       "      <td>0.546230</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.926906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.432642</td>\n",
       "      <td>0.304228</td>\n",
       "      <td>0.551569</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>0.723799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.431831</td>\n",
       "      <td>0.302675</td>\n",
       "      <td>0.550159</td>\n",
       "      <td>0.058414</td>\n",
       "      <td>0.338067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.434939</td>\n",
       "      <td>0.305555</td>\n",
       "      <td>0.552770</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.720035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.434984</td>\n",
       "      <td>0.304490</td>\n",
       "      <td>0.551806</td>\n",
       "      <td>0.020602</td>\n",
       "      <td>0.735655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.432846</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.551311</td>\n",
       "      <td>0.025261</td>\n",
       "      <td>0.678880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.429685</td>\n",
       "      <td>0.294212</td>\n",
       "      <td>0.542413</td>\n",
       "      <td>0.061532</td>\n",
       "      <td>0.312869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.434325</td>\n",
       "      <td>0.306167</td>\n",
       "      <td>0.553324</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>0.879717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.436762</td>\n",
       "      <td>0.306115</td>\n",
       "      <td>0.553277</td>\n",
       "      <td>-0.007469</td>\n",
       "      <td>0.902592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.434403</td>\n",
       "      <td>0.304960</td>\n",
       "      <td>0.552232</td>\n",
       "      <td>0.015070</td>\n",
       "      <td>0.804951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.440580</td>\n",
       "      <td>0.311011</td>\n",
       "      <td>0.557684</td>\n",
       "      <td>-0.032566</td>\n",
       "      <td>0.593503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.433493</td>\n",
       "      <td>0.306365</td>\n",
       "      <td>0.553502</td>\n",
       "      <td>-0.006820</td>\n",
       "      <td>0.911017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.433355</td>\n",
       "      <td>0.307325</td>\n",
       "      <td>0.554369</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.596809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.433696</td>\n",
       "      <td>0.303377</td>\n",
       "      <td>0.550797</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.761640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.439931</td>\n",
       "      <td>0.315513</td>\n",
       "      <td>0.561706</td>\n",
       "      <td>-0.007835</td>\n",
       "      <td>0.897844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.439204</td>\n",
       "      <td>0.311058</td>\n",
       "      <td>0.557726</td>\n",
       "      <td>-0.012174</td>\n",
       "      <td>0.841884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.431904</td>\n",
       "      <td>0.303581</td>\n",
       "      <td>0.550982</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.578471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.433724</td>\n",
       "      <td>0.305369</td>\n",
       "      <td>0.552602</td>\n",
       "      <td>0.037244</td>\n",
       "      <td>0.541539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.430706</td>\n",
       "      <td>0.299267</td>\n",
       "      <td>0.547053</td>\n",
       "      <td>0.044773</td>\n",
       "      <td>0.462938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.438018</td>\n",
       "      <td>0.309550</td>\n",
       "      <td>0.556372</td>\n",
       "      <td>-0.014751</td>\n",
       "      <td>0.808992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.442729</td>\n",
       "      <td>0.311829</td>\n",
       "      <td>0.558416</td>\n",
       "      <td>-0.038771</td>\n",
       "      <td>0.525076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.430312</td>\n",
       "      <td>0.294689</td>\n",
       "      <td>0.542852</td>\n",
       "      <td>0.034393</td>\n",
       "      <td>0.572939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration       mae       mse      rmse     corrs        ps\n",
       "0           1  0.430072  0.301220  0.548835  0.033603  0.581785\n",
       "1           2  0.427457  0.294980  0.543121  0.081537  0.180802\n",
       "2           3  0.438217  0.309540  0.556364  0.013585  0.823838\n",
       "3           4  0.432567  0.298368  0.546230  0.005599  0.926906\n",
       "4           5  0.432642  0.304228  0.551569  0.021564  0.723799\n",
       "5           6  0.431831  0.302675  0.550159  0.058414  0.338067\n",
       "6           7  0.434939  0.305555  0.552770  0.021870  0.720035\n",
       "7           8  0.434984  0.304490  0.551806  0.020602  0.735655\n",
       "8           9  0.432846  0.303944  0.551311  0.025261  0.678880\n",
       "9          10  0.429685  0.294212  0.542413  0.061532  0.312869\n",
       "10         11  0.434325  0.306167  0.553324  0.009235  0.879717\n",
       "11         12  0.436762  0.306115  0.553277 -0.007469  0.902592\n",
       "12         13  0.434403  0.304960  0.552232  0.015070  0.804951\n",
       "13         14  0.440580  0.311011  0.557684 -0.032566  0.593503\n",
       "14         15  0.433493  0.306365  0.553502 -0.006820  0.911017\n",
       "15         16  0.433355  0.307325  0.554369  0.032275  0.596809\n",
       "16         17  0.433696  0.303377  0.550797  0.018510  0.761640\n",
       "17         18  0.439931  0.315513  0.561706 -0.007835  0.897844\n",
       "18         19  0.439204  0.311058  0.557726 -0.012174  0.841884\n",
       "19         20  0.431904  0.303581  0.550982  0.033898  0.578471\n",
       "20         21  0.433724  0.305369  0.552602  0.037244  0.541539\n",
       "21         22  0.430706  0.299267  0.547053  0.044773  0.462938\n",
       "22         23  0.438018  0.309550  0.556372 -0.014751  0.808992\n",
       "23         24  0.442729  0.311829  0.558416 -0.038771  0.525076\n",
       "24         25  0.430312  0.294689  0.542852  0.034393  0.572939"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average over all iterations:\n",
      "  MAE: 0.43\n",
      "  MSE: 0.30\n",
      " RMSE: 0.55\n",
      " Corr: 0.02\n",
      "p-val: 0.6681020\n"
     ]
    }
   ],
   "source": [
    "# Resources\n",
    "## https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "\n",
    "# Store metrics for all iterations\n",
    "# aurocs = []\n",
    "maes = []    # MAE (mean absolute errors)\n",
    "mses = []    # MSE (mean squared errors)\n",
    "rmses = []   # RMSE (root mean squared errors)\n",
    "corrs = []   # spearman correlations\n",
    "ps = []      # spearman correlation p-values\n",
    "\n",
    "all_y_test_subj_out = [[] for i in range(num_iters)]\n",
    "predictions_subj_out = [[] for i in range(num_iters)]\n",
    "\n",
    "# For storing shap values across all folds\n",
    "subj_out_shap_values_0 = None\n",
    "subj_out_shap_values_1 = None\n",
    "subj_out_full_X_test = pd.DataFrame()\n",
    "\n",
    "#-----------------------------------------------#\n",
    "#      5-fold team level cross-validation       #\n",
    "#-----------------------------------------------#\n",
    "\n",
    "# For each iteration \n",
    "for i in range(num_iters):\n",
    "    print(\"Iteration: \", i+1)\n",
    "    \n",
    "    # Lists for cumulative test set and predictions for iteration\n",
    "    dfFullTest = pd.DataFrame()\n",
    "    all_y_test = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Create model for task_score prediction\n",
    "    if model == \"RFR\":\n",
    "        model_subj_out = RandomForestRegressor(n_estimators=100, random_state=1, \\\n",
    "                                           max_features='sqrt') \n",
    "    elif model == \"SVR\":\n",
    "        model_subj_out = SVR()\n",
    "        \n",
    "    elif model == \"LR\":\n",
    "        model_subj_out = LinearRegression()\n",
    "    \n",
    "    # Get fold groups\n",
    "    fold_groups = folds_dict_list[i]\n",
    "    \n",
    "    # For each fold\n",
    "    for j, (test_fold, train_fold) in enumerate(zip(test_folds, train_folds)):\n",
    "#         print(\"\\tFold: \", j+1)\n",
    "        # Get data for teams in test set\n",
    "        test_data_list = get_group_data(dfData, fold_groups[test_fold], features, 'CPS_and_ITN_mean')\n",
    "        X_test = test_data_list[0]\n",
    "        y_test = test_data_list[1]\n",
    "        all_y_test.extend(y_test.tolist())\n",
    "        dfFullTest = pd.concat([dfFullTest, test_data_list[2]], ignore_index=True)\n",
    "        \n",
    "        # Get data for teams in train set\n",
    "        train_data_list = get_group_data(dfData, fold_groups[train_fold], features, 'CPS_and_ITN_mean')\n",
    "        X_train = train_data_list[0]\n",
    "        y_train = train_data_list[1]\n",
    "        \n",
    "\n",
    "        # Train model\n",
    "        model_subj_out.fit(X_train, y_train)\n",
    "\n",
    "        # Test model\n",
    "        y_pred = model_subj_out.predict(X_test)\n",
    "        predictions.extend(y_pred.tolist())\n",
    "        \n",
    "#         ## Get SHAP values\n",
    "#         explainer = shap.TreeExplainer(model_subj_outrfc_subj_out)\n",
    "#         shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "#         if j==0:\n",
    "#             subj_out_shap_values_0 = shap_values[0]\n",
    "#             subj_out_shap_values_1 = shap_values[1]\n",
    "#         else:\n",
    "#             subj_out_shap_values_0 = np.vstack([subj_out_shap_values_0, shap_values[0]])\n",
    "#             subj_out_shap_values_1 = np.vstack([subj_out_shap_values_1, shap_values[1]])\n",
    "#         subj_out_full_X_test = pd.concat([subj_out_full_X_test, X_test], ignore_index=True)\n",
    "#         ## End of get SHAP values\n",
    "\n",
    "# ----- END OF FOLDS\n",
    "\n",
    "    # Calculate the absolute errors (MAE) of the iteration\n",
    "    predictions = np.array(predictions)\n",
    "    all_y_test = np.array(all_y_test)\n",
    "    mae = metrics.mean_absolute_error(all_y_test, predictions)\n",
    "    mse = metrics.mean_squared_error(all_y_test, predictions)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(all_y_test, predictions))\n",
    "    corr, p = spearmanr(all_y_test, predictions)\n",
    "    maes.append(mae)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    corrs.append(corr)\n",
    "    ps.append(p)\n",
    "    \n",
    "    # Save iteration subjective outcome truth labels, and predictions for stats across all iterations\n",
    "    all_y_test_subj_out[i] = all_y_test\n",
    "    predictions_subj_out[i] = predictions\n",
    "\n",
    "    # Save actual labels and predictions for iteration\n",
    "    dfTruevPred = dfFullTest.loc[:, ['GROUPID', 'block', 'CPS_and_ITN_mean']]\n",
    "    dfTruevPred['prediction'] = predictions\n",
    "    dfTruevPred.sort_values(['GROUPID', 'block', 'CPS_and_ITN_mean'], ignore_index=True, inplace=True)\n",
    "    dfTruevPred['error'] = abs(dfTruevPred['prediction'] - dfTruevPred['CPS_and_ITN_mean'])\n",
    "#     print(\"MAE from df: \", round(np.mean(dfTruevPred['error']), 2))\n",
    "    \n",
    "    if shuffled:\n",
    "        dfTruevPred.to_csv(SUBJ_OUTCOME_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_SubjOut_True_vs_Pred_SHUFF_\" + str(i+1) + \".csv\", index=False)\n",
    "    elif chance:\n",
    "        dfTruevPred.to_csv(SUBJ_OUTCOME_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_SubjOut_True_vs_Pred_CHANCE_\" + str(i+1) + \".csv\", index=False)\n",
    "    else:\n",
    "        dfTruevPred.to_csv(SUBJ_OUTCOME_RESULTS + \"RAW/\" + model + \"/\" + model + \"_SubjOut_True_vs_Pred_\" + str(i+1) + \".csv\", index=False)\n",
    "    \n",
    "#     # FOR PLOTTING ACTUAL vs. PREDICTED    \n",
    "#     dfTruevPred.plot(y=['CPS_and_ITN_mean', 'prediction'], title='Actual vs. Predicted Subjective Outcome', \\\n",
    "#                      style=['b-', 'ro'], figsize=(20, 5))\n",
    "#     # END PLOTTING\n",
    "    \n",
    "#     if model == \"RFR\":\n",
    "#         ### Compute Feature Importances for last fold iteration ###\n",
    "#         # Impurity-based importances\n",
    "#         importances = model_subj_out.feature_importances_\n",
    "#         std = np.std([tree.feature_importances_ for tree in model_subj_out.estimators_], axis=0)\n",
    "#         imps = pd.Series(importances, index=features)\n",
    "#         stds = pd.Series(std, index=features)\n",
    "#         ## Plot feature importances\n",
    "#         fig1, ax1 = plt.subplots()\n",
    "#         imps.plot.bar(yerr=std, ax=ax1)\n",
    "#         ax1.set_title(\"CPS and ITN Feature importances using MDI\")\n",
    "#         ax1.set_ylabel(\"Mean decrease in impurity\")\n",
    "#         plt.show()\n",
    "#         ## END plotting feature importances\n",
    "    \n",
    "    \n",
    "# ----- END OF ITERATIONS\n",
    "\n",
    "print(\"\\n =========== ALL ITERATIONS RESULTS SUMMARY ===========\")\n",
    "dfMetrics = pd.DataFrame({'iteration': [i for i in range(1,num_iters+1)], \\\n",
    "                          'mae': maes, 'mse': mses, 'rmse': rmses, 'corrs': corrs, 'ps': ps})\n",
    "display(dfMetrics)\n",
    "\n",
    "if shuffled:\n",
    "    dfMetrics.to_csv(SUBJ_OUTCOME_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_SubjOut_Metrics_SHUFF.csv\", index=False)\n",
    "elif chance:\n",
    "    dfMetrics.to_csv(SUBJ_OUTCOME_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_SubjOut_Metrics_CHANCE.csv\", index=False)\n",
    "else:\n",
    "    dfMetrics.to_csv(SUBJ_OUTCOME_RESULTS + \"RAW/\" + model + \"/\" + model + \"_SubjOut_Metrics.csv\", index=False)\n",
    "\n",
    "print(\"Average over all iterations:\")\n",
    "print(\"%6s %.2f\" % (\"MAE:\", np.mean(dfMetrics['mae'])))\n",
    "print(\"%6s %.2f\" % (\"MSE:\", np.mean(dfMetrics['mse'])))\n",
    "print(\"%6s %.2f\" % (\"RMSE:\", np.mean(dfMetrics['rmse'])))\n",
    "print(\"%6s %.2f\" % (\"Corr:\", np.mean(dfMetrics['corrs'])))\n",
    "print(\"%6s %.7f\" % (\"p-val:\", np.mean(dfMetrics['ps'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4afa8b",
   "metadata": {},
   "source": [
    "# Predict Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6940a075",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n",
      "Iteration:  15\n",
      "Iteration:  16\n",
      "Iteration:  17\n",
      "Iteration:  18\n",
      "Iteration:  19\n",
      "Iteration:  20\n",
      "Iteration:  21\n",
      "Iteration:  22\n",
      "Iteration:  23\n",
      "Iteration:  24\n",
      "Iteration:  25\n",
      "\n",
      " =========== ALL ITERATIONS RESULTS SUMMARY ===========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>rmse</th>\n",
       "      <th>corrs</th>\n",
       "      <th>ps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.579971</td>\n",
       "      <td>0.505046</td>\n",
       "      <td>0.710666</td>\n",
       "      <td>0.173524</td>\n",
       "      <td>0.004168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.577333</td>\n",
       "      <td>0.502446</td>\n",
       "      <td>0.708834</td>\n",
       "      <td>0.172725</td>\n",
       "      <td>0.004348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.569325</td>\n",
       "      <td>0.488178</td>\n",
       "      <td>0.698698</td>\n",
       "      <td>0.201573</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.576357</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>0.706894</td>\n",
       "      <td>0.177389</td>\n",
       "      <td>0.003390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.574005</td>\n",
       "      <td>0.497723</td>\n",
       "      <td>0.705495</td>\n",
       "      <td>0.177634</td>\n",
       "      <td>0.003345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.502187</td>\n",
       "      <td>0.708652</td>\n",
       "      <td>0.163698</td>\n",
       "      <td>0.006921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.576187</td>\n",
       "      <td>0.498469</td>\n",
       "      <td>0.706023</td>\n",
       "      <td>0.178442</td>\n",
       "      <td>0.003202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.577076</td>\n",
       "      <td>0.503183</td>\n",
       "      <td>0.709354</td>\n",
       "      <td>0.151913</td>\n",
       "      <td>0.012287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.573536</td>\n",
       "      <td>0.499623</td>\n",
       "      <td>0.706840</td>\n",
       "      <td>0.179149</td>\n",
       "      <td>0.003081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.582156</td>\n",
       "      <td>0.511069</td>\n",
       "      <td>0.714891</td>\n",
       "      <td>0.149972</td>\n",
       "      <td>0.013457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.574391</td>\n",
       "      <td>0.498940</td>\n",
       "      <td>0.706357</td>\n",
       "      <td>0.164698</td>\n",
       "      <td>0.006581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.569442</td>\n",
       "      <td>0.492381</td>\n",
       "      <td>0.701698</td>\n",
       "      <td>0.193541</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.576006</td>\n",
       "      <td>0.500350</td>\n",
       "      <td>0.707354</td>\n",
       "      <td>0.173474</td>\n",
       "      <td>0.004179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.576112</td>\n",
       "      <td>0.501370</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.162102</td>\n",
       "      <td>0.007497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.577534</td>\n",
       "      <td>0.505110</td>\n",
       "      <td>0.710711</td>\n",
       "      <td>0.150632</td>\n",
       "      <td>0.013049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.571245</td>\n",
       "      <td>0.491726</td>\n",
       "      <td>0.701232</td>\n",
       "      <td>0.195796</td>\n",
       "      <td>0.001196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.580520</td>\n",
       "      <td>0.513441</td>\n",
       "      <td>0.716548</td>\n",
       "      <td>0.147144</td>\n",
       "      <td>0.015337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.580419</td>\n",
       "      <td>0.508516</td>\n",
       "      <td>0.713103</td>\n",
       "      <td>0.155049</td>\n",
       "      <td>0.010585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.576328</td>\n",
       "      <td>0.497422</td>\n",
       "      <td>0.705281</td>\n",
       "      <td>0.161559</td>\n",
       "      <td>0.007703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.579516</td>\n",
       "      <td>0.504118</td>\n",
       "      <td>0.710012</td>\n",
       "      <td>0.148787</td>\n",
       "      <td>0.014219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.572661</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>0.703686</td>\n",
       "      <td>0.181751</td>\n",
       "      <td>0.002671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.569174</td>\n",
       "      <td>0.488149</td>\n",
       "      <td>0.698677</td>\n",
       "      <td>0.198473</td>\n",
       "      <td>0.001020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.576416</td>\n",
       "      <td>0.505883</td>\n",
       "      <td>0.711254</td>\n",
       "      <td>0.152582</td>\n",
       "      <td>0.011905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.573483</td>\n",
       "      <td>0.493957</td>\n",
       "      <td>0.702821</td>\n",
       "      <td>0.182601</td>\n",
       "      <td>0.002549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.573738</td>\n",
       "      <td>0.494025</td>\n",
       "      <td>0.702869</td>\n",
       "      <td>0.197172</td>\n",
       "      <td>0.001103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration       mae       mse      rmse     corrs        ps\n",
       "0           1  0.579971  0.505046  0.710666  0.173524  0.004168\n",
       "1           2  0.577333  0.502446  0.708834  0.172725  0.004348\n",
       "2           3  0.569325  0.488178  0.698698  0.201573  0.000846\n",
       "3           4  0.576357  0.499700  0.706894  0.177389  0.003390\n",
       "4           5  0.574005  0.497723  0.705495  0.177634  0.003345\n",
       "5           6  0.574468  0.502187  0.708652  0.163698  0.006921\n",
       "6           7  0.576187  0.498469  0.706023  0.178442  0.003202\n",
       "7           8  0.577076  0.503183  0.709354  0.151913  0.012287\n",
       "8           9  0.573536  0.499623  0.706840  0.179149  0.003081\n",
       "9          10  0.582156  0.511069  0.714891  0.149972  0.013457\n",
       "10         11  0.574391  0.498940  0.706357  0.164698  0.006581\n",
       "11         12  0.569442  0.492381  0.701698  0.193541  0.001366\n",
       "12         13  0.576006  0.500350  0.707354  0.173474  0.004179\n",
       "13         14  0.576112  0.501370  0.708075  0.162102  0.007497\n",
       "14         15  0.577534  0.505110  0.710711  0.150632  0.013049\n",
       "15         16  0.571245  0.491726  0.701232  0.195796  0.001196\n",
       "16         17  0.580520  0.513441  0.716548  0.147144  0.015337\n",
       "17         18  0.580419  0.508516  0.713103  0.155049  0.010585\n",
       "18         19  0.576328  0.497422  0.705281  0.161559  0.007703\n",
       "19         20  0.579516  0.504118  0.710012  0.148787  0.014219\n",
       "20         21  0.572661  0.495174  0.703686  0.181751  0.002671\n",
       "21         22  0.569174  0.488149  0.698677  0.198473  0.001020\n",
       "22         23  0.576416  0.505883  0.711254  0.152582  0.011905\n",
       "23         24  0.573483  0.493957  0.702821  0.182601  0.002549\n",
       "24         25  0.573738  0.494025  0.702869  0.197172  0.001103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average over all iterations:\n",
      "  MAE: 0.58\n",
      "  MSE: 0.50\n",
      " RMSE: 0.71\n",
      " Corr: 0.17\n",
      "p-val: 0.0062404\n"
     ]
    }
   ],
   "source": [
    "# Resources\n",
    "## https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "\n",
    "# Store metrics for all iterations\n",
    "# aurocs = []\n",
    "maes = []    # MAE (mean absolute errors)\n",
    "mses = []    # MSE (mean squared errors)\n",
    "rmses = []   # RMSE (root mean squared errors)\n",
    "corrs = []   # spearman correlations\n",
    "ps = []      # spearman correlation p-values\n",
    "\n",
    "all_y_test_valence = [[] for i in range(num_iters)]\n",
    "predictions_valence = [[] for i in range(num_iters)]\n",
    "\n",
    "# For storing shap values across all folds\n",
    "valence_shap_values_0 = None\n",
    "valence_shap_values_1 = None\n",
    "valence_full_X_test = pd.DataFrame()\n",
    "\n",
    "#-----------------------------------------------#\n",
    "#      5-fold team level cross-validation       #\n",
    "#-----------------------------------------------#\n",
    "\n",
    "# For each iteration \n",
    "for i in range(num_iters):\n",
    "    print(\"Iteration: \", i+1)\n",
    "    \n",
    "    # Lists for cumulative test set and predictions for iteration\n",
    "    dfFullTest = pd.DataFrame()\n",
    "    all_y_test = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Create model for task_score prediction\n",
    "    if model == \"RFR\":\n",
    "        model_valence = RandomForestRegressor(n_estimators=100, random_state=1, \\\n",
    "                                           max_features='sqrt') \n",
    "    elif model == \"SVR\":\n",
    "        model_valence = SVR()\n",
    "        \n",
    "    elif model == \"LR\":\n",
    "        model_valence = LinearRegression()\n",
    "    \n",
    "    # Get fold groups\n",
    "    fold_groups = folds_dict_list[i]\n",
    "    \n",
    "    # For each fold\n",
    "    for j, (test_fold, train_fold) in enumerate(zip(test_folds, train_folds)):\n",
    "#         print(\"\\tFold: \", j+1)\n",
    "        # Get data for teams in test set\n",
    "        test_data_list = get_group_data(dfData, fold_groups[test_fold], features, 'Valence')\n",
    "        X_test = test_data_list[0]\n",
    "        y_test = test_data_list[1]\n",
    "        all_y_test.extend(y_test.tolist())\n",
    "        dfFullTest = pd.concat([dfFullTest, test_data_list[2]], ignore_index=True)\n",
    "        \n",
    "        # Get data for teams in train set\n",
    "        train_data_list = get_group_data(dfData, fold_groups[train_fold], features, 'Valence')\n",
    "        X_train = train_data_list[0]\n",
    "        y_train = train_data_list[1]\n",
    "        \n",
    "\n",
    "        # Train model\n",
    "        model_valence.fit(X_train, y_train)\n",
    "\n",
    "        # Test model\n",
    "        y_pred = model_valence.predict(X_test)\n",
    "        predictions.extend(y_pred.tolist())\n",
    "        \n",
    "#         ## Get SHAP values\n",
    "#         explainer = shap.TreeExplainer(model_valencerfc_valence)\n",
    "#         shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "#         if j==0:\n",
    "#             valence_shap_values_0 = shap_values[0]\n",
    "#             valence_shap_values_1 = shap_values[1]\n",
    "#         else:\n",
    "#             valence_shap_values_0 = np.vstack([valence_shap_values_0, shap_values[0]])\n",
    "#             valence_shap_values_1 = np.vstack([valence_shap_values_1, shap_values[1]])\n",
    "#         valence_full_X_test = pd.concat([valence_full_X_test, X_test], ignore_index=True)\n",
    "#         ## End of get SHAP values\n",
    "\n",
    "# ----- END OF FOLDS\n",
    "\n",
    "    # Calculate the absolute errors (MAE) of the iteration\n",
    "    predictions = np.array(predictions)\n",
    "    all_y_test = np.array(all_y_test)\n",
    "    mae = metrics.mean_absolute_error(all_y_test, predictions)\n",
    "    mse = metrics.mean_squared_error(all_y_test, predictions)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(all_y_test, predictions))\n",
    "    corr, p = spearmanr(all_y_test, predictions)\n",
    "    maes.append(mae)\n",
    "    mses.append(mse)\n",
    "    rmses.append(rmse)\n",
    "    corrs.append(corr)\n",
    "    ps.append(p)\n",
    "    \n",
    "    # Save iteration valence truth labels, and predictions for stats across all iterations\n",
    "    all_y_test_subj_out[i] = all_y_test\n",
    "    predictions_subj_out[i] = predictions\n",
    "\n",
    "    # Save actual labels and predictions for iteration\n",
    "    dfTruevPred = dfFullTest.loc[:, ['GROUPID', 'block', 'Valence']]\n",
    "    dfTruevPred['prediction'] = predictions\n",
    "    dfTruevPred.sort_values(['GROUPID', 'block', 'Valence'], ignore_index=True, inplace=True)\n",
    "    dfTruevPred['error'] = abs(dfTruevPred['prediction'] - dfTruevPred['Valence'])\n",
    "#     print(\"MAE from df: \", round(np.mean(dfTruevPred['error']), 2))\n",
    "    \n",
    "    if shuffled:\n",
    "        dfTruevPred.to_csv(VALENCE_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_Valence_True_vs_Pred_SHUFF_\" + str(i+1) + \".csv\", index=False)\n",
    "    elif chance:\n",
    "        dfTruevPred.to_csv(VALENCE_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_Valence_True_vs_Pred_CHANCE_\" + str(i+1) + \".csv\", index=False)\n",
    "    else:\n",
    "        dfTruevPred.to_csv(VALENCE_RESULTS + \"RAW/\" + model + \"/\" + model + \"_Valence_True_vs_Pred_\" + str(i+1) + \".csv\", index=False)\n",
    "    \n",
    "#     # FOR PLOTTING ACTUAL vs. PREDICTED   \n",
    "#     dfTruevPred.plot(y=['Valence', 'prediction'], title='Actual vs. Predicted Valence', \\\n",
    "#                      style=['b-', 'ro'], figsize=(20, 5))\n",
    "#     # END PLOTTING\n",
    "    \n",
    "#     if model == \"RFR\":\n",
    "#         ### Compute Feature Importances for last fold iteration ###\n",
    "#         # Impurity-based importances\n",
    "#         importances = model_valence.feature_importances_\n",
    "#         std = np.std([tree.feature_importances_ for tree in model_valence.estimators_], axis=0)\n",
    "#         imps = pd.Series(importances, index=features)\n",
    "#         stds = pd.Series(std, index=features)\n",
    "#         ## Plot feature importances\n",
    "#         fig1, ax1 = plt.subplots()\n",
    "#         imps.plot.bar(yerr=std, ax=ax1)\n",
    "#         ax1.set_title(\"Valence Feature importances using MDI\")\n",
    "#         ax1.set_ylabel(\"Mean decrease in impurity\")\n",
    "#         plt.show()\n",
    "#         ## END plotting feature importances\n",
    "    \n",
    "    \n",
    "# ----- END OF ITERATIONS\n",
    "\n",
    "print(\"\\n =========== ALL ITERATIONS RESULTS SUMMARY ===========\")\n",
    "dfMetrics = pd.DataFrame({'iteration': [i for i in range(1,num_iters+1)], \\\n",
    "                          'mae': maes, 'mse': mses, 'rmse': rmses, 'corrs': corrs, 'ps': ps})\n",
    "display(dfMetrics)\n",
    "\n",
    "if shuffled:\n",
    "    dfMetrics.to_csv(VALENCE_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_Valence_Metrics_SHUFF.csv\", index=False)\n",
    "elif chance:\n",
    "    dfMetrics.to_csv(VALENCE_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_Valence_Metrics_CHANCE.csv\", index=False)\n",
    "else:\n",
    "    dfMetrics.to_csv(VALENCE_RESULTS + \"RAW/\" + model + \"/\" + model + \"_Valence_Metrics.csv\", index=False)\n",
    "\n",
    "print(\"Average over all iterations:\")\n",
    "print(\"%6s %.2f\" % (\"MAE:\", np.mean(dfMetrics['mae'])))\n",
    "print(\"%6s %.2f\" % (\"MSE:\", np.mean(dfMetrics['mse'])))\n",
    "print(\"%6s %.2f\" % (\"RMSE:\", np.mean(dfMetrics['rmse'])))\n",
    "print(\"%6s %.2f\" % (\"Corr:\", np.mean(dfMetrics['corrs'])))\n",
    "print(\"%6s %.7f\" % (\"p-val:\", np.mean(dfMetrics['ps'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e56216",
   "metadata": {},
   "source": [
    "## without cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3beae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['REC', 'DET', 'ADL', 'MDL', 'DENTR', 'LAM', 'AVL', 'MVL', 'VENTR']\n",
    "\n",
    "X = dfData.loc[:, features]  \n",
    "y = dfData.loc[:, 'task_score']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "model_task_score = RandomForestRegressor(n_estimators=100, random_state=1, max_features='sqrt')\n",
    "\n",
    "\n",
    "# Train model\n",
    "model_task_score.fit(X_train, y_train)\n",
    "    \n",
    "# Predict    \n",
    "y_pred = model_task_score.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"%5s %.2f\" % (\"MAE:\", mae))\n",
    "print(\"%5s %.2f\" % (\"MSE:\", mse))\n",
    "print(\"%5s %.2f\" % (\"RMSE:\", rmse))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c240b02",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd208c3d",
   "metadata": {},
   "source": [
    "## Predict Task Score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f39c8",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d5c1fb9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Y_PP:  [[0.35 0.65]\n",
      " [0.65 0.35]\n",
      " [0.88 0.12]\n",
      " [0.44 0.56]\n",
      " [0.21 0.79]\n",
      " [0.98 0.02]\n",
      " [0.23 0.77]\n",
      " [0.54 0.46]\n",
      " [0.73 0.27]\n",
      " [0.37 0.63]\n",
      " [0.67 0.33]\n",
      " [0.83 0.17]\n",
      " [0.35 0.65]\n",
      " [0.19 0.81]\n",
      " [0.69 0.31]\n",
      " [0.36 0.64]\n",
      " [0.43 0.57]\n",
      " [0.21 0.79]\n",
      " [0.45 0.55]\n",
      " [0.79 0.21]\n",
      " [0.57 0.43]\n",
      " [0.1  0.9 ]\n",
      " [0.43 0.57]\n",
      " [0.95 0.05]\n",
      " [0.34 0.66]\n",
      " [0.26 0.74]\n",
      " [0.47 0.53]\n",
      " [0.3  0.7 ]\n",
      " [0.53 0.47]\n",
      " [0.11 0.89]]\n",
      "Y_PP:  [[0.36 0.64]\n",
      " [0.48 0.52]\n",
      " [0.59 0.41]\n",
      " [0.49 0.51]\n",
      " [0.91 0.09]\n",
      " [0.3  0.7 ]\n",
      " [1.   0.  ]\n",
      " [0.11 0.89]\n",
      " [0.97 0.03]\n",
      " [0.47 0.53]\n",
      " [0.73 0.27]\n",
      " [0.49 0.51]\n",
      " [0.92 0.08]\n",
      " [0.93 0.07]\n",
      " [0.42 0.58]\n",
      " [0.98 0.02]\n",
      " [0.89 0.11]\n",
      " [0.76 0.24]\n",
      " [0.45 0.55]\n",
      " [0.34 0.66]\n",
      " [0.66 0.34]\n",
      " [0.91 0.09]\n",
      " [0.63 0.37]\n",
      " [0.83 0.17]\n",
      " [0.44 0.56]\n",
      " [0.83 0.17]\n",
      " [0.36 0.64]\n",
      " [0.18 0.82]\n",
      " [0.76 0.24]]\n",
      "Y_PP:  [[0.84 0.16]\n",
      " [0.87 0.13]\n",
      " [0.49 0.51]\n",
      " [0.52 0.48]\n",
      " [0.24 0.76]\n",
      " [0.58 0.42]\n",
      " [0.93 0.07]\n",
      " [0.45 0.55]\n",
      " [0.53 0.47]\n",
      " [0.52 0.48]\n",
      " [0.37 0.63]\n",
      " [0.52 0.48]\n",
      " [0.76 0.24]\n",
      " [0.51 0.49]\n",
      " [0.3  0.7 ]\n",
      " [0.67 0.33]\n",
      " [0.5  0.5 ]\n",
      " [0.13 0.87]\n",
      " [0.41 0.59]\n",
      " [0.71 0.29]\n",
      " [0.64 0.36]\n",
      " [0.8  0.2 ]\n",
      " [0.49 0.51]\n",
      " [0.83 0.17]\n",
      " [0.76 0.24]\n",
      " [0.44 0.56]\n",
      " [0.91 0.09]\n",
      " [0.53 0.47]\n",
      " [0.47 0.53]\n",
      " [0.91 0.09]]\n",
      "Y_PP:  [[0.58 0.42]\n",
      " [0.4  0.6 ]\n",
      " [0.23 0.77]\n",
      " [0.3  0.7 ]\n",
      " [0.69 0.31]\n",
      " [0.14 0.86]\n",
      " [0.84 0.16]\n",
      " [0.98 0.02]\n",
      " [0.44 0.56]\n",
      " [0.52 0.48]\n",
      " [1.   0.  ]\n",
      " [0.53 0.47]\n",
      " [0.3  0.7 ]\n",
      " [0.48 0.52]\n",
      " [0.32 0.68]\n",
      " [0.48 0.52]\n",
      " [0.22 0.78]\n",
      " [0.21 0.79]\n",
      " [0.31 0.69]\n",
      " [0.82 0.18]\n",
      " [0.52 0.48]\n",
      " [0.65 0.35]\n",
      " [0.34 0.66]\n",
      " [0.21 0.79]\n",
      " [0.35 0.65]\n",
      " [0.75 0.25]]\n",
      "Y_PP:  [[0.61 0.39]\n",
      " [0.56 0.44]\n",
      " [0.69 0.31]\n",
      " [0.42 0.58]\n",
      " [0.82 0.18]\n",
      " [0.75 0.25]\n",
      " [0.36 0.64]\n",
      " [0.84 0.16]\n",
      " [0.31 0.69]\n",
      " [0.67 0.33]\n",
      " [0.64 0.36]\n",
      " [0.48 0.52]\n",
      " [0.53 0.47]\n",
      " [0.54 0.46]\n",
      " [0.3  0.7 ]\n",
      " [0.61 0.39]\n",
      " [0.81 0.19]\n",
      " [0.66 0.34]\n",
      " [0.32 0.68]\n",
      " [0.59 0.41]\n",
      " [0.64 0.36]\n",
      " [0.2  0.8 ]\n",
      " [0.69 0.31]\n",
      " [0.53 0.47]\n",
      " [0.65 0.35]\n",
      " [0.84 0.16]]\n",
      "Y_PP:  [[0.96 0.04]\n",
      " [0.32 0.68]\n",
      " [0.35 0.65]\n",
      " [0.49 0.51]\n",
      " [0.38 0.62]\n",
      " [0.3  0.7 ]\n",
      " [0.39 0.61]\n",
      " [0.41 0.59]\n",
      " [0.39 0.61]\n",
      " [0.19 0.81]\n",
      " [0.19 0.81]\n",
      " [0.99 0.01]\n",
      " [0.62 0.38]\n",
      " [0.39 0.61]\n",
      " [0.45 0.55]\n",
      " [0.29 0.71]\n",
      " [0.49 0.51]\n",
      " [0.42 0.58]\n",
      " [0.74 0.26]\n",
      " [0.74 0.26]\n",
      " [0.36 0.64]\n",
      " [0.5  0.5 ]\n",
      " [0.46 0.54]\n",
      " [0.47 0.53]\n",
      " [0.21 0.79]\n",
      " [0.27 0.73]]\n",
      "Y_PP:  [[0.58 0.42]\n",
      " [0.7  0.3 ]\n",
      " [0.69 0.31]\n",
      " [0.15 0.85]\n",
      " [0.64 0.36]\n",
      " [0.36 0.64]\n",
      " [0.16 0.84]\n",
      " [0.31 0.69]\n",
      " [0.56 0.44]\n",
      " [0.16 0.84]\n",
      " [0.25 0.75]\n",
      " [0.67 0.33]\n",
      " [0.39 0.61]\n",
      " [0.7  0.3 ]\n",
      " [0.8  0.2 ]\n",
      " [0.26 0.74]\n",
      " [0.65 0.35]\n",
      " [0.87 0.13]\n",
      " [0.13 0.87]\n",
      " [0.4  0.6 ]\n",
      " [0.64 0.36]\n",
      " [0.15 0.85]\n",
      " [0.88 0.12]\n",
      " [0.78 0.22]\n",
      " [0.19 0.81]\n",
      " [0.33 0.67]]\n",
      "Y_PP:  [[0.65 0.35]\n",
      " [0.79 0.21]\n",
      " [0.55 0.45]\n",
      " [0.37 0.63]\n",
      " [0.26 0.74]\n",
      " [0.17 0.83]\n",
      " [0.66 0.34]\n",
      " [0.7  0.3 ]\n",
      " [0.68 0.32]\n",
      " [0.53 0.47]\n",
      " [0.66 0.34]\n",
      " [0.68 0.32]\n",
      " [0.21 0.79]\n",
      " [0.33 0.67]\n",
      " [0.23 0.77]\n",
      " [0.66 0.34]\n",
      " [0.48 0.52]\n",
      " [0.71 0.29]\n",
      " [0.28 0.72]\n",
      " [0.2  0.8 ]\n",
      " [0.37 0.63]\n",
      " [0.69 0.31]\n",
      " [0.32 0.68]\n",
      " [0.7  0.3 ]\n",
      " [0.96 0.04]\n",
      " [0.2  0.8 ]\n",
      " [0.8  0.2 ]]\n",
      "Y_PP:  [[0.82 0.18]\n",
      " [0.71 0.29]\n",
      " [0.96 0.04]\n",
      " [0.7  0.3 ]\n",
      " [0.2  0.8 ]\n",
      " [0.96 0.04]\n",
      " [0.3  0.7 ]\n",
      " [0.89 0.11]\n",
      " [0.68 0.32]\n",
      " [0.55 0.45]\n",
      " [0.15 0.85]\n",
      " [0.32 0.68]\n",
      " [0.1  0.9 ]\n",
      " [0.66 0.34]\n",
      " [0.88 0.12]\n",
      " [0.21 0.79]\n",
      " [0.74 0.26]\n",
      " [0.24 0.76]\n",
      " [0.33 0.67]\n",
      " [0.29 0.71]\n",
      " [0.79 0.21]\n",
      " [0.25 0.75]\n",
      " [0.59 0.41]\n",
      " [0.88 0.12]\n",
      " [0.39 0.61]\n",
      " [0.2  0.8 ]]\n",
      "Y_PP:  [[0.74 0.26]\n",
      " [0.17 0.83]\n",
      " [0.98 0.02]\n",
      " [0.47 0.53]\n",
      " [0.44 0.56]\n",
      " [0.79 0.21]\n",
      " [0.26 0.74]\n",
      " [0.25 0.75]\n",
      " [0.78 0.22]\n",
      " [0.56 0.44]\n",
      " [0.74 0.26]\n",
      " [0.28 0.72]\n",
      " [0.57 0.43]\n",
      " [0.68 0.32]\n",
      " [0.19 0.81]\n",
      " [0.55 0.45]\n",
      " [0.66 0.34]\n",
      " [0.7  0.3 ]\n",
      " [0.82 0.18]\n",
      " [0.29 0.71]\n",
      " [0.26 0.74]\n",
      " [0.82 0.18]\n",
      " [0.66 0.34]\n",
      " [0.62 0.38]\n",
      " [0.23 0.77]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (271, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m predict_proba_task_score[i] \u001b[38;5;241m=\u001b[39m predict_proba\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Get AUROC of iteration\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m auroc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_y_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m prec \u001b[38;5;241m=\u001b[39m precision_score(all_y_test, predictions)\n\u001b[1;32m     96\u001b[0m rec \u001b[38;5;241m=\u001b[39m recall_score(all_y_test, predictions)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:567\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    565\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[1;32m    566\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    576\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    577\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    581\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:342\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    340\u001b[0m     )\n\u001b[0;32m--> 342\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:962\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroc_curve\u001b[39m(\n\u001b[1;32m    874\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    875\u001b[0m ):\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    960\u001b[0m \n\u001b[1;32m    961\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 962\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:735\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    733\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    734\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m--> 735\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m assert_all_finite(y_true)\n\u001b[1;32m    737\u001b[0m assert_all_finite(y_score)\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/sklearn/utils/validation.py:1038\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1030\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1035\u001b[0m         )\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1040\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (271, 2) instead."
     ]
    }
   ],
   "source": [
    "# Resources\n",
    "## https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "\n",
    "# Store metrics for all iterations\n",
    "aurocs = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "all_y_test_task_score = [[] for i in range(num_iters)]\n",
    "predictions_task_score = [[] for i in range(num_iters)]\n",
    "predict_proba_task_score = [[] for i in range(num_iters)]\n",
    "\n",
    "# For storing shap values across all folds\n",
    "task_score_shap_values_0 = None\n",
    "task_score_shap_values_1 = None\n",
    "task_score_full_X_test = pd.DataFrame()\n",
    "\n",
    "#-----------------------------------------------#\n",
    "#      5-fold team level cross-validation       #\n",
    "#-----------------------------------------------#\n",
    "\n",
    "# For each iteration \n",
    "for i in range(num_iters):\n",
    "    print(\"Iteration: \", i+1)\n",
    "    \n",
    "    # Lists for cumulative test set and predictions for iteration\n",
    "    dfFullTest = pd.DataFrame()\n",
    "    all_y_test = []\n",
    "    predictions = []\n",
    "    predict_proba = []\n",
    "    \n",
    "    # Create model for task_score prediction\n",
    "    if model == \"RFC\":\n",
    "        model_task_score = RandomForestClassifier(n_estimators=100, random_state=1, max_features='sqrt') \n",
    "\n",
    "    \n",
    "    # Get fold groups\n",
    "    fold_groups = folds_dict_list[i]\n",
    "    \n",
    "    # For each fold\n",
    "    for j, (test_fold, train_fold) in enumerate(zip(test_folds, train_folds)):\n",
    "#         print(\"\\tFold: \", j+1)\n",
    "        # Get data for teams in test set\n",
    "        test_data_list = get_group_data(dfData, fold_groups[test_fold], features, 'task_score_bin')\n",
    "        X_test = test_data_list[0]\n",
    "        y_test = test_data_list[1]\n",
    "        all_y_test.extend(y_test.tolist())\n",
    "\n",
    "        dfFullTest = pd.concat([dfFullTest, test_data_list[2]], ignore_index=True)      \n",
    "        \n",
    "        # Get data for teams in train set\n",
    "        train_data_list = get_group_data(dfData, fold_groups[train_fold], features, 'task_score_bin')\n",
    "        X_train = train_data_list[0]\n",
    "        y_train = train_data_list[1]\n",
    "\n",
    "        # Train model\n",
    "        model_task_score.fit(X_train, y_train)\n",
    "\n",
    "        # Test model\n",
    "        y_pred = model_task_score.predict(X_test)\n",
    "        predictions.extend(y_pred.tolist())\n",
    "\n",
    "        y_pp = model_task_score.predict_proba(X_test)[:, 1] \n",
    "        predict_proba.extend(y_pp.tolist())\n",
    "\n",
    "        \n",
    "#         ## Get SHAP values\n",
    "#         explainer = shap.TreeExplainer(model_task_scorerfc_task_score)\n",
    "#         shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "#         if j==0:\n",
    "#             task_score_shap_values_0 = shap_values[0]\n",
    "#             task_score_shap_values_1 = shap_values[1]\n",
    "#         else:\n",
    "#             task_score_shap_values_0 = np.vstack([task_score_shap_values_0, shap_values[0]])\n",
    "#             task_score_shap_values_1 = np.vstack([task_score_shap_values_1, shap_values[1]])\n",
    "#         task_score_full_X_test = pd.concat([task_score_full_X_test, X_test], ignore_index=True)\n",
    "#         ## End of get SHAP values\n",
    "\n",
    "# ----- END OF FOLDS\n",
    "    \n",
    "    all_y_test = np.array(all_y_test)\n",
    "    all_y_test_task_score[i] = all_y_test\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    predict_proba = np.array(predict_proba)\n",
    "    \n",
    "    predictions_task_score[i] = predictions\n",
    "    predict_proba_task_score[i] = predict_proba\n",
    "    \n",
    "    \n",
    "    # Get AUROC of iteration\n",
    "    auroc = roc_auc_score(all_y_test, predict_proba)\n",
    "    prec = precision_score(all_y_test, predictions)\n",
    "    rec = recall_score(all_y_test, predictions)\n",
    "    \n",
    "    aurocs.append(auroc)\n",
    "    precision.append(prec)\n",
    "    recall.append(rec)\n",
    "\n",
    "     # Save actual labels and predictions for iteration\n",
    "    dfTruevPred = dfFullTest.loc[:, ['GROUPID', 'block', 'task_score', 'task_score_bin']]\n",
    "    dfTruevPred['prediction'] = predictions\n",
    "    dfTruevPred['predict_proba'] = predict_proba\n",
    "    dfTruevPred.sort_values(['GROUPID', 'block', 'task_score'], ignore_index=True, inplace=True)\n",
    "\n",
    "    # FOR PLOTTING ACTUAL vs. PREDICTED: https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea   \n",
    "#     cf_matrix = confusion_matrix(dfTruevPred['task_score_bin'], dfTruevPred['prediction'])\n",
    "#     sns.heatmap(cf_matrix, annot=True)\n",
    "    # END PLOTTING\n",
    "        \n",
    "    \n",
    "    if shuffled:\n",
    "        dfTruevPred.to_csv(TASK_SCORE_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_TaskScore_True_vs_Pred_SHUFF_\" + str(i+1) + \".csv\", index=False)\n",
    "    elif chance:\n",
    "        dfTruevPred.to_csv(TASK_SCORE_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_TaskScore_True_vs_Pred_CHANCE_\" + str(i+1) + \".csv\", index=False)\n",
    "    else:\n",
    "        dfTruevPred.to_csv(TASK_SCORE_RESULTS + \"RAW/\" + model + \"/\" + model + \"_TaskScore_True_vs_Pred_\" + str(i+1) + \".csv\", index=False)\n",
    "  \n",
    "\n",
    "    \n",
    "#     if (model == \"RFR\") or (model == \"RFC\"):    \n",
    "#         ### Compute Feature Importances for last fold iteration ###\n",
    "#         # Impurity-based importances\n",
    "#         importances = model_task_score.feature_importances_\n",
    "#         std = np.std([tree.feature_importances_ for tree in model_task_score.estimators_], axis=0)\n",
    "#         imps = pd.Series(importances, index=features)\n",
    "#         stds = pd.Series(std, index=features)\n",
    "#         ## Plot feature importances\n",
    "#         fig1, ax1 = plt.subplots()\n",
    "#         imps.plot.bar(yerr=std, ax=ax1)\n",
    "#         ax1.set_title(\"Task Score Feature importances using MDI\")\n",
    "#         ax1.set_ylabel(\"Mean decrease in impurity\")\n",
    "#         plt.show()\n",
    "#         ## END plotting feature importances\n",
    "    \n",
    "    \n",
    "# ----- END OF ITERATIONS\n",
    "\n",
    "print(\"\\n =========== ALL ITERATIONS RESULTS SUMMARY ===========\")\n",
    "dfMetrics = pd.DataFrame({'iteration': [i for i in range(1,num_iters+1)], \\\n",
    "                          'auroc': aurocs, 'precision': precision, 'recall': recall})\n",
    "\n",
    "display(dfMetrics)\n",
    "\n",
    "if shuffled:\n",
    "    dfMetrics.to_csv(TASK_SCORE_RESULTS + \"RAW_SHUFFLED/\" + model + \"/\" + model + \"_TaskScore_Metrics_SHUFF.csv\", index=False)\n",
    "elif chance:\n",
    "    dfMetrics.to_csv(TASK_SCORE_RESULTS + \"RAW_CHANCE/\" + model + \"/\" + model + \"_TaskScore_Metrics_CHANCE.csv\", index=False)\n",
    "else:\n",
    "    dfMetrics.to_csv(TASK_SCORE_RESULTS + \"RAW/\" + model + \"/\" + model + \"_TaskScore_Metrics.csv\", index=False)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"%12s %.2f\" % (\"AUROC:\", np.mean(dfMetrics['auroc'])))\n",
    "print(\"%12s %.2f\" % (\"Precision:\", np.mean(dfMetrics['precision'])))\n",
    "print(\"%12s %.2f\" % (\"Recall:\", np.mean(dfMetrics['recall'])))\n",
    "\n",
    "\n",
    "print(\"%6s %.2f\" % (\"Med AUROC:\", np.median(dfMetrics['auroc'])))\n",
    "\n",
    "# Get median iterations\n",
    "med_auroc = np.median(aurocs)\n",
    "med_auroc_idx = np.argsort(aurocs)[len(aurocs)//2]\n",
    "\n",
    "# Plot confusion matrix of median iteration\n",
    "cf_matrix = confusion_matrix(all_y_test_task_score[med_auroc_idx], predictions_task_score[med_auroc_idx])\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['Zero', 'One']\n",
    "make_confusion_matrix(cf_matrix, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='Blues')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a90d5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271,)\n",
      "(271,)\n"
     ]
    }
   ],
   "source": [
    "# Get median iterations\n",
    "med_auroc = np.median(aurocs)\n",
    "med_auroc_idx = np.argsort(aurocs)[len(aurocs)//2]\n",
    "\n",
    "print(all_y_test_task_score[med_auroc_idx].shape)\n",
    "print(predictions_task_score[med_auroc_idx].shape)\n",
    "\n",
    "# # Plot confusion matrix of median iteration\n",
    "# cf_matrix = confusion_matrix(all_y_test[med_auroc_idx], predictions_task_score[med_auroc_idx])\n",
    "# labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "# categories = ['Zero', 'One']\n",
    "# make_confusion_matrix(cf_matrix, \n",
    "#                       group_names=labels,\n",
    "#                       categories=categories, \n",
    "#                       cmap='Blues')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fba86",
   "metadata": {},
   "source": [
    "### OHE labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc6294",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc474d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = ['REC', 'DET', 'ADL', 'MDL', 'DENTR', 'LAM', 'AVL', 'MVL', 'VENTR']\n",
    "\n",
    "X = dfData.loc[:, features]  \n",
    "y = dfData.loc[:, 'task_score_cat']\n",
    "\n",
    "# y_bin = label_binarize(y, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n",
    "\n",
    "model_task_score = RandomForestClassifier(n_estimators=100, random_state=1, \\\n",
    "                                           max_features='sqrt', class_weight='balanced') \n",
    "\n",
    "# Train model\n",
    "model_task_score.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "y_pred = model_task_score.predict(X_test)\n",
    "\n",
    "# display(pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred}))\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix, annot=True)\n",
    "\n",
    "\n",
    "# #roc auc score: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "# y_pp = model_task_score.predict_proba(X_test) \n",
    "# print(\"\\ny_train info\")\n",
    "# print(\"total len: \", len(y_train))\n",
    "# print(\"unique vals: \", len(set(y_train)))\n",
    "# print(\"\\ny_test info\")\n",
    "# print(\"total len: \", len(y_test))\n",
    "# print(\"unique vals: \", len(set(y_test)))\n",
    "# print(\"\\ny_pp info\")\n",
    "# print(\"y_pp shape: \", y_pp.shape)\n",
    "\n",
    "# auroc = roc_auc_score(y_test, y_pp, multi_class='ovo', average='weighted')\n",
    "# print(auroc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8125a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9b0121e",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac616f44",
   "metadata": {},
   "source": [
    "## Random Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc81f9",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe173050",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['REC', 'DET', 'ADL', 'MDL', 'DENTR', 'LAM', 'AVL', 'MVL', 'VENTR']\n",
    "\n",
    "X = dfData.loc[:, features]  \n",
    "y = dfData.loc[:, 'task_score_cat']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "## Use the random grid to search for best hyperparameters\n",
    "\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "pprint(rf_random.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators=100, random_state=1, max_features='sqrt') \n",
    "base_model.fit(X_train, y_train)\n",
    "base_predictions = base_model.predict(X_test)\n",
    "\n",
    "print('Base Model Performance')\n",
    "print(classification_report(y_test, base_predictions, zero_division=1))\n",
    "\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_predictions = best_random.predict(X_test)\n",
    "\n",
    "print('Random Model Performance')\n",
    "print(classification_report(y_test, random_predictions, zero_division=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db72d82",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef54b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "## https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfData.loc[:, features], dfData.loc[:, 'task_score'], \\\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "\n",
    "if model == \"RFR\":\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    pprint(random_grid)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ## Use the random grid to search for best hyperparameters\n",
    "    \n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, y_train)\n",
    "\n",
    "    pprint(rf_random.best_params_)\n",
    "\n",
    "    base_model = RandomForestRegressor(n_estimators=100, random_state=1, max_features='sqrt') \n",
    "    base_model.fit(X_train, y_train)\n",
    "    base_predictions = base_model.predict(X_test)\n",
    "    base_mae = metrics.mean_absolute_error(y_test, base_predictions)\n",
    "\n",
    "    print(\"===== Random Forest Regessor =====\")\n",
    "    print('Base Model Performance')\n",
    "    print('MAE: ', np.round(base_mae, 2))\n",
    "\n",
    "\n",
    "    best_random = rf_random.best_estimator_\n",
    "    random_predictions = best_random.predict(X_test)\n",
    "    random_mae = metrics.mean_absolute_error(y_test, random_predictions)\n",
    "\n",
    "    print('Random Model Performance')\n",
    "    print('MAE: ', np.round(random_mae, 2))\n",
    "\n",
    "\n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (random_mae - base_mae) / base_mae))\n",
    "    \n",
    "elif model == \"SVR\":\n",
    "    kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    \n",
    "    degree = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    gamma = ['scale', 'auto', 0.1]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'kernel': kernel,\n",
    "                   'degree': degree,\n",
    "                   'gamma': gamma}\n",
    "    pprint(random_grid)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ## Use the random grid to search for best hyperparameters\n",
    "    \n",
    "    # First create the base model to tune\n",
    "    sv = SVR()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    sv_random = RandomizedSearchCV(estimator = sv, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    sv_random.fit(X_train, y_train)\n",
    "\n",
    "    pprint(sv_random.best_params_)\n",
    "\n",
    "    base_model = SVR() \n",
    "    base_model.fit(X_train, y_train)\n",
    "    base_predictions = base_model.predict(X_test)\n",
    "    base_mae = metrics.mean_absolute_error(y_test, base_predictions)\n",
    "\n",
    "    print(\"===== Support Vector Machine Regessor =====\")\n",
    "    print('Base Model Performance')\n",
    "    print('MAE: ', np.round(base_mae, 2))\n",
    "\n",
    "\n",
    "    best_random = sv_random.best_estimator_\n",
    "    random_predictions = best_random.predict(X_test)\n",
    "    random_mae = metrics.mean_absolute_error(y_test, random_predictions)\n",
    "\n",
    "    print('Random Model Performance')\n",
    "    print('MAE: ', np.round(random_mae, 2))\n",
    "\n",
    "\n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (random_mae - base_mae) / base_mae))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3a0f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29255b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a614206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Define fold names\n",
    "# train_folds = []\n",
    "# test_folds = []\n",
    "# set_type = \"test\"\n",
    "# for j in range(1,num_folds+1): \n",
    "#     col_name = \"Fold\" + str(j) + \"_\" + set_type\n",
    "#     test_folds.append(col_name)\n",
    "#     set_type = \"train\"  \n",
    "#     col_name = \"Fold\" + str(j) + \"_\" + set_type\n",
    "#     train_folds.append(col_name)\n",
    "#     set_type = \"test\"\n",
    "\n",
    "# # print(\"Train fold names: \", train_folds)\n",
    "# # print(\"Test fold names: \",test_folds)\n",
    "\n",
    "\n",
    "# folds_dict_list = []\n",
    "\n",
    "# # Split teams into 5 groups\n",
    "# teams = pd.unique(dfMeasures.GROUPID)\n",
    "\n",
    "# # For every iteration\n",
    "# for i in range(1,num_iters+1):\n",
    "#     print(\"\\n\\n=============Iteration: \", i)\n",
    "#     random.Random(i).shuffle(teams)\n",
    "\n",
    "# #     teams = shuffle(teams, random_state=i)\n",
    "\n",
    "#     groups = np.array_split(teams, num_folds)\n",
    "#     print(\"\\ngroups: \")\n",
    "#     for k,grp in enumerate(groups):\n",
    "#         print(k, grp)\n",
    "    \n",
    "#     # Define groups for each fold\n",
    "#     fold_groups = {}\n",
    "#     for j, (train_fold, test_fold) in enumerate(zip(train_folds, test_folds)):\n",
    "#         # make the current group the test group\n",
    "#         fold_groups[test_fold] = groups[j]\n",
    "#         # make all other groups the train group\n",
    "#         train_group = groups[:j] + groups[j+1:]\n",
    "#         train_group = [team for group in train_group for team in group]\n",
    "#         fold_groups[train_fold] = train_group\n",
    "        \n",
    "#     ## Confirm that for each fold, there is no team overlap bewteen train and test set\n",
    "#     for j in range(1,num_folds+1):\n",
    "#         assert set(fold_groups['Fold'+str(j)+'_test']).isdisjoint(set(fold_groups['Fold'+str(j)+'_train'])), \"There is overlap in train and test set \" + str(j)\n",
    "    \n",
    "#     print(\"* No team overlap *\")\n",
    "    \n",
    "#     print(\"\\n!!!!! FOLD GROUPS BEFORE IT GOES BAD\")\n",
    "#     for kei in fold_groups:\n",
    "#         print(\"\\n\", kei)\n",
    "#         print(fold_groups[kei])\n",
    "\n",
    "        \n",
    "#     if i == 2:\n",
    "#         print(\"\\n BEFORE APPEND folds_dict_list[i-2] index = \", i-2)\n",
    "#         folds_dict = folds_dict_list[i-2]\n",
    "#         for key in folds_dict:\n",
    "#             print(\"\\n\", key)\n",
    "#             print(folds_dict[key])\n",
    "      \n",
    "#     # Add fold groups to dictionary\n",
    "#     folds_dict_list.append(fold_groups)\n",
    "    \n",
    "#     if i == 2:\n",
    "#         print(\"\\n AFTER APPEND folds_dict_list[i-2] index = \", i-2)\n",
    "#         folds_dict = folds_dict_list[i-2]\n",
    "#         for key in folds_dict:\n",
    "#             print(\"\\n\", key)\n",
    "#             print(folds_dict[key])   \n",
    "\n",
    "# # Informational\n",
    "# # print(\"\\nNumber of iterations: \", len(folds_dict_list))\n",
    "\n",
    "# print(\"\\n*~*~*~*~*~* iterate through folds_dict_list: \")\n",
    "# for i,dicti in enumerate(folds_dict_list):\n",
    "#     print(\"\\n list index: \", i)\n",
    "#     for j in range(1,num_folds+1):\n",
    "#         assert set(dicti['Fold'+str(j)+'_test']).isdisjoint(set(dicti['Fold'+str(j)+'_train'])), \"There is overlap in train and test set \" + str(j)\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
